<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.1" />
<title>shoji.io API documentation</title>
<meta name="description" content="Internal low-level I/O routines, not intended for end users." />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>shoji.io</code></h1>
</header>
<section id="section-intro">
<p>Internal low-level I/O routines, not intended for end users.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Internal low-level I/O routines, not intended for end users.
&#34;&#34;&#34;

from typing import Union, Optional, Tuple, Dict, List
import fdb
import numpy as np
import shoji
import numba
import pickle
import copy


CHUNK_SIZE = 2_000


@fdb.transactional
def get_entity(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, name: str) -&gt; Optional[str]:
        s = get_subspace(tr, wsm, name)
        if s is not None:
                return s
        d = get_dimension(tr, wsm, name)
        if d is not None:
                return d
        t = get_tensor(tr, wsm, name)
        if t is not None:
                return t
        return None


@fdb.transactional
def get_subspace(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, name: str) -&gt; Optional[shoji.WorkspaceManager]:
        subspace = wsm._subspace
        if subspace.exists(tr, name):
                child = subspace.open(tr, name)
                wsm = shoji.WorkspaceManager(wsm._db, child, wsm._path + (name,))
                wsm._name = name
                return wsm
        return None


@fdb.transactional
def get_dimension(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, name: str) -&gt; Optional[shoji.Dimension]:
        subspace = wsm._subspace
        val = tr[subspace[&#34;dimensions&#34;][name]]
        if val.present():
                val = tr[subspace[&#34;dimensions&#34;][name]]
                dim = shoji.Dimension(shape=int.from_bytes(val[:8], &#34;little&#34;, signed=True), length=int.from_bytes(val[8:], &#34;little&#34;, signed=False))
                dim.name = name
                dim.wsm = wsm
                return dim
        return None


@fdb.transactional
def get_tensor(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, name: str) -&gt; Optional[shoji.Tensor]:
        subspace = wsm._subspace
        # (&#34;tensors&#34;, name) = (tensor.dtype, tensor.rank, tensor.jagged) + tensor.dims + shape
        # where shape[0] == -1 if jagged
        val = tr[subspace.pack((&#34;tensors&#34;, name))]
        if val.present():
                t = pickle.loads(val.value)
                if t[1] &gt; 0:
                        shape = t[-t[1]:]
                else:
                        shape = ()
                tensor = shoji.Tensor(t[0], t[3:3 + t[1]], shape=shape)
                tensor.jagged = t[2] == 1
                tensor.name = name
                tensor.wsm = wsm
                return tensor
        return None


@fdb.transactional
def delete_entity(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, name: str) -&gt; None:
        subspace = wsm._subspace
        if subspace.exists(tr, name):
                subspace.open(tr, name).remove(tr)
        elif tr[subspace[&#34;dimensions&#34;][name]].present():
                tr.clear_range_startswith(subspace[&#34;dimensions&#34;][name].key())
        elif tr[subspace[&#34;tensors&#34;][name]].present():
                tr.clear_range_startswith(subspace[&#34;tensors&#34;][name].key())
                tr.clear_range_startswith(subspace[&#34;tensor_values&#34;][name].key())


@fdb.transactional
def create_or_update_dimension(tr, wsm: shoji.WorkspaceManager, name: str, dim: shoji.Dimension):
        subspace = wsm._subspace
        # Check that name doesn&#39;t already exist
        existing = get_entity(tr, wsm, name)
        if existing is not None:
                if not isinstance(existing, shoji.Dimension):
                        raise AttributeError(f&#34;Name already exists (as {existing})&#34;)
                # Update an existing dimension
                prev_dim = existing
                if prev_dim.shape != dim.shape:
                        if isinstance(dim.shape, int):
                                # Changing to a fixed shape, so we must check that all relevant tensors agree
                                all_tensors: List[str] = [subspace[&#34;tensors&#34;].unpack(k)[0] for k,v in tr[subspace[&#34;tensors&#34;].range()]]
                                for tensor_name in all_tensors:
                                        tensor = get_tensor(tr, wsm, tensor_name)
                                        if tensor.rank &gt; 0 and tensor.dims[0] == name:
                                                if tensor.shape[0] != dim.shape:
                                                        raise AttributeError(f&#34;New shape of existing dimension &#39;{name}&#39; conflicts with length {tensor.shape[0]} of existing tensor &#39;{tensor_name}&#39;&#34;)
                                dim.length = dim.shape
                        else:
                                dim.length = prev_dim.length
        # Create or update the dimension
        tr[subspace[&#34;dimensions&#34;][name]] = (dim.shape if dim.shape is not None else -1).to_bytes(8, &#34;little&#34;, signed=True) + dim.length.to_bytes(8, &#34;little&#34;, signed=False)


@fdb.transactional
def create_or_update_tensor(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, name: str, tensor: shoji.Tensor) -&gt; None:
        # Check that name doesn&#39;t already exist
        existing = get_entity(tr, wsm, name)
        if existing is not None:
                if not isinstance(existing, shoji.Tensor):
                        raise AttributeError(f&#34;Name already exists (as {existing})&#34;)
        else:
                # Check that the dimensions of the tensor exist
                for ix, d in enumerate(tensor.dims):
                        if isinstance(d, str):
                                dim = get_dimension(tr, wsm, d)
                                if dim is None:
                                        raise KeyError(f&#34;Tensor dimension &#39;{d}&#39; is not defined&#34;)
                                if dim.shape is not None and tensor.inits is not None:
                                        if tensor.jagged:
                                                assert isinstance(tensor.inits, list)
                                                for row in tensor.inits:
                                                        if row.shape[ix] != dim.shape:
                                                                raise ValueError(f&#34;Tensor &#39;{name}&#39; dimension {ix} (&#39;{tensor.dims[ix]}&#39;) must be exactly {dim.shape} elements long&#34;)
                                        elif dim.shape != tensor.inits.shape[ix]:  # type: ignore
                                                raise ValueError(f&#34;Tensor &#39;{name}&#39; dimension {ix} (&#39;{tensor.dims[ix]}&#39;) must be exactly {dim.shape} elements long&#34;)
                                # Check that the dimensions of the tensor match the shape of the tensor
                                if dim.shape is None:
                                        if ix &gt; 0:
                                                tensor.jagged = True
                                else:
                                        if tensor.inits is not None and tensor.shape[ix] != dim.shape:
                                                raise IndexError(f&#34;Mismatch between the declared shape {dim.shape} of dimension &#39;{d}&#39; and the shape {tensor.shape} of values&#34;)
        # Store tensor definition (overwriting any existing definition)
        # (&#34;tensors&#34;, name) = (dtype, rank, jagged) + dims + shape  
        # where shape[0] == -1 if jagged, and tuple value is encoded using pickle
        subspace = wsm._subspace
        shape = tensor.shape if existing is not None else (0,) + tensor.shape[1:]  # if this is a new tensor, use shape (0, ...)
        tr[subspace.pack((&#34;tensors&#34;, name))] = pickle.dumps((tensor.dtype, tensor.rank, 1 if tensor.jagged else 0) + tensor.dims + shape)


def coerce_dtype(dtype, v) -&gt; Union[int, float, bool, str]:
        if dtype in (&#34;uint8&#34;, &#34;uint16&#34;, &#34;uint32&#34;, &#34;uint64&#34;, &#34;int8&#34;, &#34;int16&#34;, &#34;int32&#34;, &#34;int64&#34;):
                return int(v)
        elif dtype in (&#34;float16&#34;, &#34;float32&#34;, &#34;float64&#34;):
                return float(v)
        elif dtype == &#34;bool&#34;:
                return bool(v)
        elif dtype == &#34;string&#34;:
                return str(v)
        else:
                raise TypeError()


@fdb.transactional
def write_tensor_values(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, name: str, in_tensor: shoji.Tensor, indices: np.ndarray = None):
        &#34;&#34;&#34;
        Args:
                tr          Transaction
                wsm         WorkspaceManager
                name        Name of the tensor in the database
                in_tensor      A tensor with inits 
                indices     A vector of row indices where the inits should be written, or None to append at end of tensor
        &#34;&#34;&#34;
        subspace = wsm._subspace
        tensor = get_tensor(tr, wsm, name)
        codec = shoji.Codec(tensor.dtype)
        assert in_tensor.inits is not None

        if tensor.rank == 0:  # It&#39;s a scalar value
                key = subspace.pack((&#34;tensor_values&#34;, name) + (0, 0))
                tr[key] = codec.encode(np.array(in_tensor.inits))
                return
                
        is_update = True  
        if indices is None:  # We&#39;re appending to the end of the tensor
                is_update = False
                new_length = len(in_tensor) + tensor.shape[0]
                indices = np.arange(tensor.shape[0], new_length)
                # Update the tensor length
                new_tensor = copy.copy(tensor)
                new_tensor.shape = (new_length,) + in_tensor.shape[1:]
                create_or_update_tensor(tr, wsm, name, new_tensor)

        # Update the index
        if tensor.rank == 1:
                if is_update:
                        old_vals = read_tensor_values(tr, wsm, name, tensor, indices)
                        for i, ix in enumerate(indices):
                                key = subspace.pack((&#34;tensor_indexes&#34;, name, coerce_dtype(tensor.dtype, old_vals[i]), int(ix)))
                                del tr[key]

                for i, value in enumerate(in_tensor.inits):
                        key = subspace.pack((&#34;tensor_indexes&#34;, name, coerce_dtype(tensor.dtype, value), int(indices[i])))
                        tr[key] = b&#39;&#39;

        if not tensor.jagged:
                assert(isinstance(in_tensor.inits, np.ndarray))
                rows_per_chunk = max(1, int(np.floor(CHUNK_SIZE / (in_tensor.inits.size // in_tensor.inits.shape[0]))))
                if rows_per_chunk &gt; 1:
                        chunks = indices // rows_per_chunk
                        for chunk in np.unique(chunks):
                                vals = in_tensor.inits[chunks == chunk]
                                if len(vals) &lt; rows_per_chunk:
                                        # Need to read the previous tensor values and update them first
                                        prev = tr[subspace.pack((&#34;tensor_values&#34;, name, int(chunk), 0))]
                                        if prev.present():
                                                prev_vals = codec.decode(prev.value)
                                        else:
                                                prev_vals = np.zeros((rows_per_chunk,) + in_tensor.shape[1:], dtype=tensor.numpy_dtype())
                                        ixs = indices[chunks == chunk]
                                        prev_vals[np.mod(ixs, rows_per_chunk)] = vals
                                        vals = prev_vals
                                key = subspace.pack((&#34;tensor_values&#34;, name, int(chunk), 0))
                                tr[key] = codec.encode(vals)
                        return
                # Falls through to the code below

        # Jagged or only one row per chunk
        for i, ix in enumerate(indices):
                encoded = codec.encode(np.array(in_tensor.inits[i]))
                for j in range(0, len(encoded), CHUNK_SIZE):
                        key = subspace.pack((&#34;tensor_values&#34;, name, int(ix), j // CHUNK_SIZE))
                        tr[key] = encoded[j:j + CHUNK_SIZE]


@numba.jit
def compute_ranges(elements):
    elements = np.sort(elements)
    ranges = []
    start = elements[0]
    for ix in range(1, len(elements)):
        if elements[ix] != elements[ix - 1] + 1:
            ranges.append((start, elements[ix - 1]))
            start = elements[ix]
    ranges.append((start, elements[-1]))
    return ranges


@fdb.transactional
def read_chunked_rows(tr: fdb.impl.Transaction, subspace: fdb.subspace_impl.Subspace, name: str, i: int, j: int, codec: shoji.Codec) -&gt; List[np.ndarray]:
        start = subspace.range((&#34;tensor_values&#34;, name, i)).start
        stop = subspace.range((&#34;tensor_values&#34;, name, j)).stop
        result = []
        ix = i
        encoded = bytearray()
        # TODO: use parallelism with futures instead
        for k, v in tr.get_range(start, stop, streaming_mode=fdb.StreamingMode.want_all):
                row = subspace.unpack(k)[-2]
                if row != ix:
                        result.append(codec.decode(bytes(encoded)))
                        encoded = bytearray()
                        ix = row
                encoded += v
        result.append(codec.decode(bytes(encoded)))
        return result


@fdb.transactional
def read_tensor_values(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, name: str, tensor: shoji.Tensor = None, indices: np.ndarray = None) -&gt; np.ndarray:
        subspace = wsm._subspace
        if tensor is None:
                tensor = get_tensor(tr, wsm, name)

        # Convert the list of indices to ranges as far as possible
        if indices is None:
                if tensor.rank &gt; 0:
                        n_rows = tensor.shape[0]
                        indices = np.arange(0, n_rows)
                        ranges = [(0, n_rows)]
                else:
                        n_rows = 1
                        indices = np.array([0], dtype=&#34;int&#34;)
                        ranges = [(0, 0)]
        else:
                n_rows = len(indices)
                ranges = compute_ranges(indices)

        codec = shoji.Codec(tensor.dtype)

        if tensor.rank == 0:  # It&#39;s a scalar value
                key = subspace.pack((&#34;tensor_values&#34;, name) + (0, 0))
                return codec.decode(tr[key].value).item()

        if tensor.jagged:
                resultj: List[np.ndarray] = []
                for (start, stop) in ranges:
                        resultj += read_chunked_rows(tr, subspace, name, start, stop, codec)
                return resultj
        rows_per_chunk = max(1, int(np.floor(CHUNK_SIZE / (np.prod(tensor.shape) // tensor.shape[0]))))
        if rows_per_chunk == 1:
                result = np.empty((n_rows,) + tensor.shape[1:], dtype=tensor.numpy_dtype())
                ix = 0
                for (start, stop) in ranges:
                        vals = read_chunked_rows(tr, subspace, name, start, stop, codec)
                        result[ix: ix + len(vals)] = vals
                return result
        else:  # A dense array (not jagged or scalar) with more than one row per chunk
                chunks = indices // rows_per_chunk
                result = np.empty((n_rows,) + tensor.shape[1:], dtype=tensor.numpy_dtype())
                i = 0
                # Use parallelism with futures
                r = {}
                unique_chunks = np.unique(chunks)
                for chunk in unique_chunks:
                        key = subspace.pack((&#34;tensor_values&#34;, name, int(chunk), 0))
                        r[chunk] = tr[key]  # This returns a Future
                for chunk in unique_chunks:
                        vals = codec.decode(r[chunk].value)  # This blocks reading the Future
                        ixs = indices[chunks == chunk]
                        vals = vals[np.mod(ixs, rows_per_chunk)]  # Extract the relevant rows from the chunk
                        result[i: i + len(vals)] = vals
                        i += len(vals)
                return result


@fdb.transactional
def const_compare(tr, wsm: shoji.WorkspaceManager, name: str, operator: str, const: Tuple[int, str, float]) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Compare a tensor to a constant value, and return all indices that match
        &#34;&#34;&#34;
        # Code for range, equality and inequality filters
        tensor = wsm[name]
        assert isinstance(tensor, shoji.Tensor)
        const = tensor.python_dtype()(const)  # Cast the const to string, float or int
        index = wsm._subspace[&#34;tensor_indexes&#34;][name]
        eq_range = index[const].range()
        all_range = index.range()
        start, stop = all_range.start, all_range.stop
        if operator == &#34;==&#34;:
                start, stop = eq_range.start, eq_range.stop
        if operator == &#34;&gt;=&#34;:
                start = eq_range.start
        elif operator == &#34;&gt;&#34;:
                start = tr.get_key(fdb.KeySelector.first_greater_than(index[const]))
        elif operator == &#34;&lt;=&#34;:
                stop = eq_range.stop
        elif operator == &#34;&lt;&#34;:
                stop = tr.get_key(fdb.KeySelector.last_less_than(index[const]))
        return np.array([index.unpack(k)[1] for k, _ in tr[start:stop]], dtype=&#34;int64&#34;)


@fdb.transactional
def append_tensors(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, dname: str, vals: Dict[str, Union[List[np.ndarray], np.ndarray]]) -&gt; None:
        &#34;&#34;&#34;
        Append values to a set of named tensors, which share their first dimension

        Args:
                tr: Transaction
                wsm: `shoji.workspace.WorkspaceManager`
                dname: Name of the dimension along which the values should be appended
                vals: dict of tensor names and corresponding values (np.ndarray or list of np.ndarray)

        Remarks:
                The function will check for the existence of all the tensors, that they have the same first dimension,
                and that no other tensor in the workspace has the same first dimension. It will also check that the values
                given have the same length along the first dimension, and that all other dimensions of each tensor match the 
                definitions of those tensors.
        &#34;&#34;&#34;
        subspace = wsm._subspace
        tensors: Dict[str, shoji.Tensor] = {}

        # Check that all named tensors exist, and have the right first dimension
        for name in vals.keys():
                t = get_tensor(tr, wsm, name)
                if t is None:
                        raise NameError(f&#34;Tensor &#39;{name}&#39; does not exist in the workspace&#34;)
                tensors[name] = t  # type: ignore
                if tensors[name].rank == 0 or tensors[name].dims[0] != dname:
                        raise ValueError(f&#34;Tensor &#39;{name}&#39; does not have &#39;{dname}&#39; as first dimension&#34;)

        # Check the rank of the values
        new_length = 0
        for name, values in vals.items():
                if tensors[name].jagged:
                        for row in values:
                                if tensors[name].rank != row.ndim + 1:  # type: ignore
                                        raise ValueError(f&#34;Tensor &#39;{name}&#39; of rank {tensors[name].rank} cannot be initialized with rank-{row.ndim + 1} array&#34;)  # type: ignore
                else:
                        if tensors[name].rank != values.ndim:  # type: ignore
                                raise ValueError(f&#34;Tensor &#39;{name}&#39; of rank {tensors[name].rank} cannot be initialized with rank-{values.ndim} array&#34;)  # type: ignore
                new_length = tensors[name].shape[0] + len(values)

        # Check that all relevant tensors will have the right shape after appending
        all_tensors: List[str] = [subspace[&#34;tensors&#34;].unpack(k)[0] for k,v in tr[subspace[&#34;tensors&#34;].range()]]
        for tensor_name in all_tensors:
                if tensor_name not in vals:
                        tensor: shoji.Tensor = wsm[tensor_name]  # type: ignore
                        if tensor.rank != 0 and tensor.dims[0] == dname:
                                if tensor.shape[0] != new_length:
                                        raise ValueError(f&#34;Length {tensor.shape[0]} of tensor &#39;{tensor.name}&#39; would conflict with dimension &#39;{dname}&#39; length {new_length} after appending {&#39;,&#39;.join(vals.keys())}&#34;)

        # Check that all the other dimensions are the correct shape according to their definitions
        for name, values in vals.items():
                for i, idim in enumerate(tensors[name].dims):
                        if i == 0 or idim is None:
                                continue
                        elif isinstance(idim, int):  # Anonymous fixed-shape dimension
                                target_size = idim
                        elif isinstance(idim, str):  # Named dimension
                                dim = get_dimension(tr, wsm, idim)
                                if dim is None:
                                        raise KeyError(f&#34;Dimension {idim} is undefined&#34;)
                                if dim.shape is None:
                                        continue
                                target_size = dim.shape
                        if tensors[name].jagged:
                                for row in values:
                                        if row.shape[i - 1] != target_size: 
                                                raise ValueError(f&#34;Tensor &#39;{name}&#39; dimension {i} (&#39;{idim}&#39;) must be exactly {target_size} elements long, but shape was {row.shape}&#34;)
                        elif target_size != values.shape[i]:  # type: ignore
                                raise ValueError(f&#34;Tensor &#39;{name}&#39; dimension {i} (&#39;{idim}&#39;) must be exactly {target_size} elements long, but shape was {row.shape}&#34;)
                                
        # Write the values
        for name, values in vals.items():
                tensors[name].inits = values
                if isinstance(values, (list, tuple)):
                        tensors[name].shape = (len(values), ) + (None, ) * (values[0].ndim - 1)  # TODO: handle jagged tensors
                else:
                        tensors[name].shape = values.shape  # TODO: handle jagged tensors
                write_tensor_values(tr, wsm, name, tensors[name])

        # Update the first dimension
        dim = wsm[dname]  # type: ignore
        dim.length = dim.length + len(values)
        create_or_update_dimension(tr, wsm, dname, dim)


def __nil__():
        pass</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="shoji.io.append_tensors"><code class="name flex">
<span>def <span class="ident">append_tensors</span></span>(<span>tr: fdb.impl.Transaction, wsm: <a title="shoji.workspace.WorkspaceManager" href="workspace.html#shoji.workspace.WorkspaceManager">WorkspaceManager</a>, dname: str, vals: Dict[str, Union[List[numpy.ndarray], numpy.ndarray]]) -> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Append values to a set of named tensors, which share their first dimension</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tr</code></strong></dt>
<dd>Transaction</dd>
<dt><strong><code>wsm</code></strong></dt>
<dd><code><a title="shoji.workspace.WorkspaceManager" href="workspace.html#shoji.workspace.WorkspaceManager">WorkspaceManager</a></code></dd>
<dt><strong><code>dname</code></strong></dt>
<dd>Name of the dimension along which the values should be appended</dd>
<dt><strong><code>vals</code></strong></dt>
<dd>dict of tensor names and corresponding values (np.ndarray or list of np.ndarray)</dd>
</dl>
<h2 id="remarks">Remarks</h2>
<p>The function will check for the existence of all the tensors, that they have the same first dimension,
and that no other tensor in the workspace has the same first dimension. It will also check that the values
given have the same length along the first dimension, and that all other dimensions of each tensor match the
definitions of those tensors.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@fdb.transactional
def append_tensors(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, dname: str, vals: Dict[str, Union[List[np.ndarray], np.ndarray]]) -&gt; None:
        &#34;&#34;&#34;
        Append values to a set of named tensors, which share their first dimension

        Args:
                tr: Transaction
                wsm: `shoji.workspace.WorkspaceManager`
                dname: Name of the dimension along which the values should be appended
                vals: dict of tensor names and corresponding values (np.ndarray or list of np.ndarray)

        Remarks:
                The function will check for the existence of all the tensors, that they have the same first dimension,
                and that no other tensor in the workspace has the same first dimension. It will also check that the values
                given have the same length along the first dimension, and that all other dimensions of each tensor match the 
                definitions of those tensors.
        &#34;&#34;&#34;
        subspace = wsm._subspace
        tensors: Dict[str, shoji.Tensor] = {}

        # Check that all named tensors exist, and have the right first dimension
        for name in vals.keys():
                t = get_tensor(tr, wsm, name)
                if t is None:
                        raise NameError(f&#34;Tensor &#39;{name}&#39; does not exist in the workspace&#34;)
                tensors[name] = t  # type: ignore
                if tensors[name].rank == 0 or tensors[name].dims[0] != dname:
                        raise ValueError(f&#34;Tensor &#39;{name}&#39; does not have &#39;{dname}&#39; as first dimension&#34;)

        # Check the rank of the values
        new_length = 0
        for name, values in vals.items():
                if tensors[name].jagged:
                        for row in values:
                                if tensors[name].rank != row.ndim + 1:  # type: ignore
                                        raise ValueError(f&#34;Tensor &#39;{name}&#39; of rank {tensors[name].rank} cannot be initialized with rank-{row.ndim + 1} array&#34;)  # type: ignore
                else:
                        if tensors[name].rank != values.ndim:  # type: ignore
                                raise ValueError(f&#34;Tensor &#39;{name}&#39; of rank {tensors[name].rank} cannot be initialized with rank-{values.ndim} array&#34;)  # type: ignore
                new_length = tensors[name].shape[0] + len(values)

        # Check that all relevant tensors will have the right shape after appending
        all_tensors: List[str] = [subspace[&#34;tensors&#34;].unpack(k)[0] for k,v in tr[subspace[&#34;tensors&#34;].range()]]
        for tensor_name in all_tensors:
                if tensor_name not in vals:
                        tensor: shoji.Tensor = wsm[tensor_name]  # type: ignore
                        if tensor.rank != 0 and tensor.dims[0] == dname:
                                if tensor.shape[0] != new_length:
                                        raise ValueError(f&#34;Length {tensor.shape[0]} of tensor &#39;{tensor.name}&#39; would conflict with dimension &#39;{dname}&#39; length {new_length} after appending {&#39;,&#39;.join(vals.keys())}&#34;)

        # Check that all the other dimensions are the correct shape according to their definitions
        for name, values in vals.items():
                for i, idim in enumerate(tensors[name].dims):
                        if i == 0 or idim is None:
                                continue
                        elif isinstance(idim, int):  # Anonymous fixed-shape dimension
                                target_size = idim
                        elif isinstance(idim, str):  # Named dimension
                                dim = get_dimension(tr, wsm, idim)
                                if dim is None:
                                        raise KeyError(f&#34;Dimension {idim} is undefined&#34;)
                                if dim.shape is None:
                                        continue
                                target_size = dim.shape
                        if tensors[name].jagged:
                                for row in values:
                                        if row.shape[i - 1] != target_size: 
                                                raise ValueError(f&#34;Tensor &#39;{name}&#39; dimension {i} (&#39;{idim}&#39;) must be exactly {target_size} elements long, but shape was {row.shape}&#34;)
                        elif target_size != values.shape[i]:  # type: ignore
                                raise ValueError(f&#34;Tensor &#39;{name}&#39; dimension {i} (&#39;{idim}&#39;) must be exactly {target_size} elements long, but shape was {row.shape}&#34;)
                                
        # Write the values
        for name, values in vals.items():
                tensors[name].inits = values
                if isinstance(values, (list, tuple)):
                        tensors[name].shape = (len(values), ) + (None, ) * (values[0].ndim - 1)  # TODO: handle jagged tensors
                else:
                        tensors[name].shape = values.shape  # TODO: handle jagged tensors
                write_tensor_values(tr, wsm, name, tensors[name])

        # Update the first dimension
        dim = wsm[dname]  # type: ignore
        dim.length = dim.length + len(values)
        create_or_update_dimension(tr, wsm, dname, dim)</code></pre>
</details>
</dd>
<dt id="shoji.io.coerce_dtype"><code class="name flex">
<span>def <span class="ident">coerce_dtype</span></span>(<span>dtype, v) -> Union[int, float, bool, str]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def coerce_dtype(dtype, v) -&gt; Union[int, float, bool, str]:
        if dtype in (&#34;uint8&#34;, &#34;uint16&#34;, &#34;uint32&#34;, &#34;uint64&#34;, &#34;int8&#34;, &#34;int16&#34;, &#34;int32&#34;, &#34;int64&#34;):
                return int(v)
        elif dtype in (&#34;float16&#34;, &#34;float32&#34;, &#34;float64&#34;):
                return float(v)
        elif dtype == &#34;bool&#34;:
                return bool(v)
        elif dtype == &#34;string&#34;:
                return str(v)
        else:
                raise TypeError()</code></pre>
</details>
</dd>
<dt id="shoji.io.compute_ranges"><code class="name flex">
<span>def <span class="ident">compute_ranges</span></span>(<span>elements)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@numba.jit
def compute_ranges(elements):
    elements = np.sort(elements)
    ranges = []
    start = elements[0]
    for ix in range(1, len(elements)):
        if elements[ix] != elements[ix - 1] + 1:
            ranges.append((start, elements[ix - 1]))
            start = elements[ix]
    ranges.append((start, elements[-1]))
    return ranges</code></pre>
</details>
</dd>
<dt id="shoji.io.const_compare"><code class="name flex">
<span>def <span class="ident">const_compare</span></span>(<span>tr, wsm: <a title="shoji.workspace.WorkspaceManager" href="workspace.html#shoji.workspace.WorkspaceManager">WorkspaceManager</a>, name: str, operator: str, const: Tuple[int, str, float]) -> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Compare a tensor to a constant value, and return all indices that match</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@fdb.transactional
def const_compare(tr, wsm: shoji.WorkspaceManager, name: str, operator: str, const: Tuple[int, str, float]) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Compare a tensor to a constant value, and return all indices that match
        &#34;&#34;&#34;
        # Code for range, equality and inequality filters
        tensor = wsm[name]
        assert isinstance(tensor, shoji.Tensor)
        const = tensor.python_dtype()(const)  # Cast the const to string, float or int
        index = wsm._subspace[&#34;tensor_indexes&#34;][name]
        eq_range = index[const].range()
        all_range = index.range()
        start, stop = all_range.start, all_range.stop
        if operator == &#34;==&#34;:
                start, stop = eq_range.start, eq_range.stop
        if operator == &#34;&gt;=&#34;:
                start = eq_range.start
        elif operator == &#34;&gt;&#34;:
                start = tr.get_key(fdb.KeySelector.first_greater_than(index[const]))
        elif operator == &#34;&lt;=&#34;:
                stop = eq_range.stop
        elif operator == &#34;&lt;&#34;:
                stop = tr.get_key(fdb.KeySelector.last_less_than(index[const]))
        return np.array([index.unpack(k)[1] for k, _ in tr[start:stop]], dtype=&#34;int64&#34;)</code></pre>
</details>
</dd>
<dt id="shoji.io.create_or_update_dimension"><code class="name flex">
<span>def <span class="ident">create_or_update_dimension</span></span>(<span>tr, wsm: <a title="shoji.workspace.WorkspaceManager" href="workspace.html#shoji.workspace.WorkspaceManager">WorkspaceManager</a>, name: str, dim: <a title="shoji.dimension.Dimension" href="dimension.html#shoji.dimension.Dimension">Dimension</a>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@fdb.transactional
def create_or_update_dimension(tr, wsm: shoji.WorkspaceManager, name: str, dim: shoji.Dimension):
        subspace = wsm._subspace
        # Check that name doesn&#39;t already exist
        existing = get_entity(tr, wsm, name)
        if existing is not None:
                if not isinstance(existing, shoji.Dimension):
                        raise AttributeError(f&#34;Name already exists (as {existing})&#34;)
                # Update an existing dimension
                prev_dim = existing
                if prev_dim.shape != dim.shape:
                        if isinstance(dim.shape, int):
                                # Changing to a fixed shape, so we must check that all relevant tensors agree
                                all_tensors: List[str] = [subspace[&#34;tensors&#34;].unpack(k)[0] for k,v in tr[subspace[&#34;tensors&#34;].range()]]
                                for tensor_name in all_tensors:
                                        tensor = get_tensor(tr, wsm, tensor_name)
                                        if tensor.rank &gt; 0 and tensor.dims[0] == name:
                                                if tensor.shape[0] != dim.shape:
                                                        raise AttributeError(f&#34;New shape of existing dimension &#39;{name}&#39; conflicts with length {tensor.shape[0]} of existing tensor &#39;{tensor_name}&#39;&#34;)
                                dim.length = dim.shape
                        else:
                                dim.length = prev_dim.length
        # Create or update the dimension
        tr[subspace[&#34;dimensions&#34;][name]] = (dim.shape if dim.shape is not None else -1).to_bytes(8, &#34;little&#34;, signed=True) + dim.length.to_bytes(8, &#34;little&#34;, signed=False)</code></pre>
</details>
</dd>
<dt id="shoji.io.create_or_update_tensor"><code class="name flex">
<span>def <span class="ident">create_or_update_tensor</span></span>(<span>tr: fdb.impl.Transaction, wsm: <a title="shoji.workspace.WorkspaceManager" href="workspace.html#shoji.workspace.WorkspaceManager">WorkspaceManager</a>, name: str, tensor: <a title="shoji.tensor.Tensor" href="tensor.html#shoji.tensor.Tensor">Tensor</a>) -> NoneType</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@fdb.transactional
def create_or_update_tensor(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, name: str, tensor: shoji.Tensor) -&gt; None:
        # Check that name doesn&#39;t already exist
        existing = get_entity(tr, wsm, name)
        if existing is not None:
                if not isinstance(existing, shoji.Tensor):
                        raise AttributeError(f&#34;Name already exists (as {existing})&#34;)
        else:
                # Check that the dimensions of the tensor exist
                for ix, d in enumerate(tensor.dims):
                        if isinstance(d, str):
                                dim = get_dimension(tr, wsm, d)
                                if dim is None:
                                        raise KeyError(f&#34;Tensor dimension &#39;{d}&#39; is not defined&#34;)
                                if dim.shape is not None and tensor.inits is not None:
                                        if tensor.jagged:
                                                assert isinstance(tensor.inits, list)
                                                for row in tensor.inits:
                                                        if row.shape[ix] != dim.shape:
                                                                raise ValueError(f&#34;Tensor &#39;{name}&#39; dimension {ix} (&#39;{tensor.dims[ix]}&#39;) must be exactly {dim.shape} elements long&#34;)
                                        elif dim.shape != tensor.inits.shape[ix]:  # type: ignore
                                                raise ValueError(f&#34;Tensor &#39;{name}&#39; dimension {ix} (&#39;{tensor.dims[ix]}&#39;) must be exactly {dim.shape} elements long&#34;)
                                # Check that the dimensions of the tensor match the shape of the tensor
                                if dim.shape is None:
                                        if ix &gt; 0:
                                                tensor.jagged = True
                                else:
                                        if tensor.inits is not None and tensor.shape[ix] != dim.shape:
                                                raise IndexError(f&#34;Mismatch between the declared shape {dim.shape} of dimension &#39;{d}&#39; and the shape {tensor.shape} of values&#34;)
        # Store tensor definition (overwriting any existing definition)
        # (&#34;tensors&#34;, name) = (dtype, rank, jagged) + dims + shape  
        # where shape[0] == -1 if jagged, and tuple value is encoded using pickle
        subspace = wsm._subspace
        shape = tensor.shape if existing is not None else (0,) + tensor.shape[1:]  # if this is a new tensor, use shape (0, ...)
        tr[subspace.pack((&#34;tensors&#34;, name))] = pickle.dumps((tensor.dtype, tensor.rank, 1 if tensor.jagged else 0) + tensor.dims + shape)</code></pre>
</details>
</dd>
<dt id="shoji.io.delete_entity"><code class="name flex">
<span>def <span class="ident">delete_entity</span></span>(<span>tr: fdb.impl.Transaction, wsm: <a title="shoji.workspace.WorkspaceManager" href="workspace.html#shoji.workspace.WorkspaceManager">WorkspaceManager</a>, name: str) -> NoneType</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@fdb.transactional
def delete_entity(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, name: str) -&gt; None:
        subspace = wsm._subspace
        if subspace.exists(tr, name):
                subspace.open(tr, name).remove(tr)
        elif tr[subspace[&#34;dimensions&#34;][name]].present():
                tr.clear_range_startswith(subspace[&#34;dimensions&#34;][name].key())
        elif tr[subspace[&#34;tensors&#34;][name]].present():
                tr.clear_range_startswith(subspace[&#34;tensors&#34;][name].key())
                tr.clear_range_startswith(subspace[&#34;tensor_values&#34;][name].key())</code></pre>
</details>
</dd>
<dt id="shoji.io.get_dimension"><code class="name flex">
<span>def <span class="ident">get_dimension</span></span>(<span>tr: fdb.impl.Transaction, wsm: <a title="shoji.workspace.WorkspaceManager" href="workspace.html#shoji.workspace.WorkspaceManager">WorkspaceManager</a>, name: str) -> Union[<a title="shoji.dimension.Dimension" href="dimension.html#shoji.dimension.Dimension">Dimension</a>, NoneType]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@fdb.transactional
def get_dimension(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, name: str) -&gt; Optional[shoji.Dimension]:
        subspace = wsm._subspace
        val = tr[subspace[&#34;dimensions&#34;][name]]
        if val.present():
                val = tr[subspace[&#34;dimensions&#34;][name]]
                dim = shoji.Dimension(shape=int.from_bytes(val[:8], &#34;little&#34;, signed=True), length=int.from_bytes(val[8:], &#34;little&#34;, signed=False))
                dim.name = name
                dim.wsm = wsm
                return dim
        return None</code></pre>
</details>
</dd>
<dt id="shoji.io.get_entity"><code class="name flex">
<span>def <span class="ident">get_entity</span></span>(<span>tr: fdb.impl.Transaction, wsm: <a title="shoji.workspace.WorkspaceManager" href="workspace.html#shoji.workspace.WorkspaceManager">WorkspaceManager</a>, name: str) -> Union[str, NoneType]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@fdb.transactional
def get_entity(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, name: str) -&gt; Optional[str]:
        s = get_subspace(tr, wsm, name)
        if s is not None:
                return s
        d = get_dimension(tr, wsm, name)
        if d is not None:
                return d
        t = get_tensor(tr, wsm, name)
        if t is not None:
                return t
        return None</code></pre>
</details>
</dd>
<dt id="shoji.io.get_subspace"><code class="name flex">
<span>def <span class="ident">get_subspace</span></span>(<span>tr: fdb.impl.Transaction, wsm: <a title="shoji.workspace.WorkspaceManager" href="workspace.html#shoji.workspace.WorkspaceManager">WorkspaceManager</a>, name: str) -> Union[<a title="shoji.workspace.WorkspaceManager" href="workspace.html#shoji.workspace.WorkspaceManager">WorkspaceManager</a>, NoneType]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@fdb.transactional
def get_subspace(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, name: str) -&gt; Optional[shoji.WorkspaceManager]:
        subspace = wsm._subspace
        if subspace.exists(tr, name):
                child = subspace.open(tr, name)
                wsm = shoji.WorkspaceManager(wsm._db, child, wsm._path + (name,))
                wsm._name = name
                return wsm
        return None</code></pre>
</details>
</dd>
<dt id="shoji.io.get_tensor"><code class="name flex">
<span>def <span class="ident">get_tensor</span></span>(<span>tr: fdb.impl.Transaction, wsm: <a title="shoji.workspace.WorkspaceManager" href="workspace.html#shoji.workspace.WorkspaceManager">WorkspaceManager</a>, name: str) -> Union[<a title="shoji.tensor.Tensor" href="tensor.html#shoji.tensor.Tensor">Tensor</a>, NoneType]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@fdb.transactional
def get_tensor(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, name: str) -&gt; Optional[shoji.Tensor]:
        subspace = wsm._subspace
        # (&#34;tensors&#34;, name) = (tensor.dtype, tensor.rank, tensor.jagged) + tensor.dims + shape
        # where shape[0] == -1 if jagged
        val = tr[subspace.pack((&#34;tensors&#34;, name))]
        if val.present():
                t = pickle.loads(val.value)
                if t[1] &gt; 0:
                        shape = t[-t[1]:]
                else:
                        shape = ()
                tensor = shoji.Tensor(t[0], t[3:3 + t[1]], shape=shape)
                tensor.jagged = t[2] == 1
                tensor.name = name
                tensor.wsm = wsm
                return tensor
        return None</code></pre>
</details>
</dd>
<dt id="shoji.io.read_chunked_rows"><code class="name flex">
<span>def <span class="ident">read_chunked_rows</span></span>(<span>tr: fdb.impl.Transaction, subspace: fdb.subspace_impl.Subspace, name: str, i: int, j: int, codec: <a title="shoji.codec.Codec" href="codec.html#shoji.codec.Codec">Codec</a>) -> List[numpy.ndarray]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@fdb.transactional
def read_chunked_rows(tr: fdb.impl.Transaction, subspace: fdb.subspace_impl.Subspace, name: str, i: int, j: int, codec: shoji.Codec) -&gt; List[np.ndarray]:
        start = subspace.range((&#34;tensor_values&#34;, name, i)).start
        stop = subspace.range((&#34;tensor_values&#34;, name, j)).stop
        result = []
        ix = i
        encoded = bytearray()
        # TODO: use parallelism with futures instead
        for k, v in tr.get_range(start, stop, streaming_mode=fdb.StreamingMode.want_all):
                row = subspace.unpack(k)[-2]
                if row != ix:
                        result.append(codec.decode(bytes(encoded)))
                        encoded = bytearray()
                        ix = row
                encoded += v
        result.append(codec.decode(bytes(encoded)))
        return result</code></pre>
</details>
</dd>
<dt id="shoji.io.read_tensor_values"><code class="name flex">
<span>def <span class="ident">read_tensor_values</span></span>(<span>tr: fdb.impl.Transaction, wsm: <a title="shoji.workspace.WorkspaceManager" href="workspace.html#shoji.workspace.WorkspaceManager">WorkspaceManager</a>, name: str, tensor: <a title="shoji.tensor.Tensor" href="tensor.html#shoji.tensor.Tensor">Tensor</a> = None, indices: numpy.ndarray = None) -> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@fdb.transactional
def read_tensor_values(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, name: str, tensor: shoji.Tensor = None, indices: np.ndarray = None) -&gt; np.ndarray:
        subspace = wsm._subspace
        if tensor is None:
                tensor = get_tensor(tr, wsm, name)

        # Convert the list of indices to ranges as far as possible
        if indices is None:
                if tensor.rank &gt; 0:
                        n_rows = tensor.shape[0]
                        indices = np.arange(0, n_rows)
                        ranges = [(0, n_rows)]
                else:
                        n_rows = 1
                        indices = np.array([0], dtype=&#34;int&#34;)
                        ranges = [(0, 0)]
        else:
                n_rows = len(indices)
                ranges = compute_ranges(indices)

        codec = shoji.Codec(tensor.dtype)

        if tensor.rank == 0:  # It&#39;s a scalar value
                key = subspace.pack((&#34;tensor_values&#34;, name) + (0, 0))
                return codec.decode(tr[key].value).item()

        if tensor.jagged:
                resultj: List[np.ndarray] = []
                for (start, stop) in ranges:
                        resultj += read_chunked_rows(tr, subspace, name, start, stop, codec)
                return resultj
        rows_per_chunk = max(1, int(np.floor(CHUNK_SIZE / (np.prod(tensor.shape) // tensor.shape[0]))))
        if rows_per_chunk == 1:
                result = np.empty((n_rows,) + tensor.shape[1:], dtype=tensor.numpy_dtype())
                ix = 0
                for (start, stop) in ranges:
                        vals = read_chunked_rows(tr, subspace, name, start, stop, codec)
                        result[ix: ix + len(vals)] = vals
                return result
        else:  # A dense array (not jagged or scalar) with more than one row per chunk
                chunks = indices // rows_per_chunk
                result = np.empty((n_rows,) + tensor.shape[1:], dtype=tensor.numpy_dtype())
                i = 0
                # Use parallelism with futures
                r = {}
                unique_chunks = np.unique(chunks)
                for chunk in unique_chunks:
                        key = subspace.pack((&#34;tensor_values&#34;, name, int(chunk), 0))
                        r[chunk] = tr[key]  # This returns a Future
                for chunk in unique_chunks:
                        vals = codec.decode(r[chunk].value)  # This blocks reading the Future
                        ixs = indices[chunks == chunk]
                        vals = vals[np.mod(ixs, rows_per_chunk)]  # Extract the relevant rows from the chunk
                        result[i: i + len(vals)] = vals
                        i += len(vals)
                return result</code></pre>
</details>
</dd>
<dt id="shoji.io.write_tensor_values"><code class="name flex">
<span>def <span class="ident">write_tensor_values</span></span>(<span>tr: fdb.impl.Transaction, wsm: <a title="shoji.workspace.WorkspaceManager" href="workspace.html#shoji.workspace.WorkspaceManager">WorkspaceManager</a>, name: str, in_tensor: <a title="shoji.tensor.Tensor" href="tensor.html#shoji.tensor.Tensor">Tensor</a>, indices: numpy.ndarray = None)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<p>tr
Transaction
wsm
WorkspaceManager
name
Name of the tensor in the database
in_tensor
A tensor with inits
indices
A vector of row indices where the inits should be written, or None to append at end of tensor</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@fdb.transactional
def write_tensor_values(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, name: str, in_tensor: shoji.Tensor, indices: np.ndarray = None):
        &#34;&#34;&#34;
        Args:
                tr          Transaction
                wsm         WorkspaceManager
                name        Name of the tensor in the database
                in_tensor      A tensor with inits 
                indices     A vector of row indices where the inits should be written, or None to append at end of tensor
        &#34;&#34;&#34;
        subspace = wsm._subspace
        tensor = get_tensor(tr, wsm, name)
        codec = shoji.Codec(tensor.dtype)
        assert in_tensor.inits is not None

        if tensor.rank == 0:  # It&#39;s a scalar value
                key = subspace.pack((&#34;tensor_values&#34;, name) + (0, 0))
                tr[key] = codec.encode(np.array(in_tensor.inits))
                return
                
        is_update = True  
        if indices is None:  # We&#39;re appending to the end of the tensor
                is_update = False
                new_length = len(in_tensor) + tensor.shape[0]
                indices = np.arange(tensor.shape[0], new_length)
                # Update the tensor length
                new_tensor = copy.copy(tensor)
                new_tensor.shape = (new_length,) + in_tensor.shape[1:]
                create_or_update_tensor(tr, wsm, name, new_tensor)

        # Update the index
        if tensor.rank == 1:
                if is_update:
                        old_vals = read_tensor_values(tr, wsm, name, tensor, indices)
                        for i, ix in enumerate(indices):
                                key = subspace.pack((&#34;tensor_indexes&#34;, name, coerce_dtype(tensor.dtype, old_vals[i]), int(ix)))
                                del tr[key]

                for i, value in enumerate(in_tensor.inits):
                        key = subspace.pack((&#34;tensor_indexes&#34;, name, coerce_dtype(tensor.dtype, value), int(indices[i])))
                        tr[key] = b&#39;&#39;

        if not tensor.jagged:
                assert(isinstance(in_tensor.inits, np.ndarray))
                rows_per_chunk = max(1, int(np.floor(CHUNK_SIZE / (in_tensor.inits.size // in_tensor.inits.shape[0]))))
                if rows_per_chunk &gt; 1:
                        chunks = indices // rows_per_chunk
                        for chunk in np.unique(chunks):
                                vals = in_tensor.inits[chunks == chunk]
                                if len(vals) &lt; rows_per_chunk:
                                        # Need to read the previous tensor values and update them first
                                        prev = tr[subspace.pack((&#34;tensor_values&#34;, name, int(chunk), 0))]
                                        if prev.present():
                                                prev_vals = codec.decode(prev.value)
                                        else:
                                                prev_vals = np.zeros((rows_per_chunk,) + in_tensor.shape[1:], dtype=tensor.numpy_dtype())
                                        ixs = indices[chunks == chunk]
                                        prev_vals[np.mod(ixs, rows_per_chunk)] = vals
                                        vals = prev_vals
                                key = subspace.pack((&#34;tensor_values&#34;, name, int(chunk), 0))
                                tr[key] = codec.encode(vals)
                        return
                # Falls through to the code below

        # Jagged or only one row per chunk
        for i, ix in enumerate(indices):
                encoded = codec.encode(np.array(in_tensor.inits[i]))
                for j in range(0, len(encoded), CHUNK_SIZE):
                        key = subspace.pack((&#34;tensor_values&#34;, name, int(ix), j // CHUNK_SIZE))
                        tr[key] = encoded[j:j + CHUNK_SIZE]</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="shoji" href="index.html">shoji</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="shoji.io.append_tensors" href="#shoji.io.append_tensors">append_tensors</a></code></li>
<li><code><a title="shoji.io.coerce_dtype" href="#shoji.io.coerce_dtype">coerce_dtype</a></code></li>
<li><code><a title="shoji.io.compute_ranges" href="#shoji.io.compute_ranges">compute_ranges</a></code></li>
<li><code><a title="shoji.io.const_compare" href="#shoji.io.const_compare">const_compare</a></code></li>
<li><code><a title="shoji.io.create_or_update_dimension" href="#shoji.io.create_or_update_dimension">create_or_update_dimension</a></code></li>
<li><code><a title="shoji.io.create_or_update_tensor" href="#shoji.io.create_or_update_tensor">create_or_update_tensor</a></code></li>
<li><code><a title="shoji.io.delete_entity" href="#shoji.io.delete_entity">delete_entity</a></code></li>
<li><code><a title="shoji.io.get_dimension" href="#shoji.io.get_dimension">get_dimension</a></code></li>
<li><code><a title="shoji.io.get_entity" href="#shoji.io.get_entity">get_entity</a></code></li>
<li><code><a title="shoji.io.get_subspace" href="#shoji.io.get_subspace">get_subspace</a></code></li>
<li><code><a title="shoji.io.get_tensor" href="#shoji.io.get_tensor">get_tensor</a></code></li>
<li><code><a title="shoji.io.read_chunked_rows" href="#shoji.io.read_chunked_rows">read_chunked_rows</a></code></li>
<li><code><a title="shoji.io.read_tensor_values" href="#shoji.io.read_tensor_values">read_tensor_values</a></code></li>
<li><code><a title="shoji.io.write_tensor_values" href="#shoji.io.write_tensor_values">write_tensor_values</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>