<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.1" />
<title>shoji.io API documentation</title>
<meta name="description" content="Internal low-level I/O routines, not intended for end users." />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>shoji.io</code></h1>
</header>
<section id="section-intro">
<p>Internal low-level I/O routines, not intended for end users.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Internal low-level I/O routines, not intended for end users.
&#34;&#34;&#34;

from typing import Union, Optional, Tuple, Dict, List
import fdb
import numpy as np
import shoji
import numba
import struct


@fdb.transactional
def name_exists(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, name: str) -&gt; Optional[str]:
        subspace = wsm._subspace
        if subspace.exists(tr, wsm._path + (name,)):
                return &#34;Workspace&#34;
        if tr[subspace[&#34;dimensions&#34;][name]].present():
                return &#34;Dimension&#34;
        for _ in tr[subspace.range((&#34;tensors&#34;, name))]:
                return &#34;Tensor&#34;
        return None

@fdb.transactional
def read_entity(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, name: str) -&gt; Union[shoji.Dimension, shoji.WorkspaceManager, shoji.Tensor, None]:
        subspace = wsm._subspace
        if subspace.exists(tr, name):
                child = subspace.open(tr, name)
                return shoji.WorkspaceManager(wsm._db, child, wsm._path + (name,))
        elif tr[subspace[&#34;dimensions&#34;][name]].present():
                val = tr[subspace[&#34;dimensions&#34;][name]]
                dim = shoji.Dimension(shape=int.from_bytes(val[:8], &#34;little&#34;, signed=True), length=int.from_bytes(val[8:], &#34;little&#34;, signed=False))
                dim.assigned_name = name
                dim.wsm = wsm
                return dim
        else:
                # key_tuple = (&#34;tensors&#34;, name, tensor.dtype, tensor.rank) + tensor.dims + (jagged, length)
                tensor_tuples = tr[subspace.range((&#34;tensors&#34;, name))]
                for k, _ in tensor_tuples:
                        key = subspace.unpack(k)
                        tensor = shoji.Tensor(key[2], key[4:4+key[3]], length=key[-1])
                        tensor.jagged = key[-2] == 1
                        tensor.assigned_name = name
                        tensor.wsm = wsm
                        return tensor
        raise KeyError(f&#34;{name} not found&#34;)

@fdb.transactional
def delete_entity(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, name: str) -&gt; None:
        subspace = wsm._subspace
        if subspace.exists(tr, name):
                subspace.open(tr, name).remove(tr)
        elif tr[subspace[&#34;dimensions&#34;][name]].present():
                tr.clear_range_startswith(subspace[&#34;dimensions&#34;][name].key())
        elif tr[subspace[&#34;tensors&#34;][name]].present():
                tr.clear_range_startswith(subspace[&#34;tensors&#34;][name].key())
                tr.clear_range_startswith(subspace[&#34;tensor_values&#34;][name].key())


@fdb.transactional
def create_or_update_dimension(tr, wsm: shoji.WorkspaceManager, name: str, dim: shoji.Dimension):
        subspace = wsm._subspace
        # Check that name doesn&#39;t already exist
        existing_name = name_exists(tr, wsm, name)
        if existing_name is not None:
                if existing_name != &#34;Dimension&#34;:
                        raise AttributeError(f&#34;Name already exists (as {existing_name})&#34;)
                # Update an existing dimension
                prev_dim: shoji.Dimension = read_entity(tr, wsm, name)
                if prev_dim.shape != dim.shape:
                        raise AttributeError(f&#34;Cannot modify shape of existing dimension &#39;{name}&#39;&#34;)
        # Create or update the dimension
        tr[subspace[&#34;dimensions&#34;][name]] = (dim.shape if dim.shape is not None else -1).to_bytes(8, &#34;little&#34;, signed=True) + dim.length.to_bytes(8, &#34;little&#34;, signed=False)


@fdb.transactional
def create_tensor(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, name: str, tensor: shoji.Tensor) -&gt; None:
        subspace = wsm._subspace
        # Check that name doesn&#39;t already exist
        existing_name = name_exists(tr, wsm, name)
        if existing_name is not None:
                raise AttributeError(f&#34;Name already exists (as {existing_name})&#34;)
        # Check that the dimensions of the tensor exist
        for ix, d in enumerate(tensor.dims):
                if isinstance(d, str):
                        dim = wsm[d]
                        assert isinstance(dim, shoji.Dimension)
                        # Check that the dimensions of the tensor match the shape of the tensor
                        if dim.shape is None:
                                if ix &gt; 0:
                                        tensor.jagged = True
                        else:
                                if tensor.inits is not None and tensor.shape[ix] != dim.shape:
                                        raise IndexError(f&#34;Mismatch between the declared shape {dim.shape} of dimension &#39;{d}&#39; and the shape {tensor.shape} of values&#34;)
        # Store tensor definition
        # (&#34;tensors&#34;, name, dtype, rank) + dims + (jagged, length)
        key_tuple = (&#34;tensors&#34;, name, tensor.dtype, tensor.rank) + tensor.dims + (1 if tensor.jagged else 0, 0)
        key = subspace.pack(key_tuple)
        tr[key] = b&#39;&#39;


def coerce_dtype(dtype, v) -&gt; Union[int, float, bool, str]:
        if dtype in (&#34;uint8&#34;, &#34;uint16&#34;, &#34;uint32&#34;, &#34;uint64&#34;, &#34;int8&#34;, &#34;int16&#34;, &#34;int32&#34;, &#34;int64&#34;):
                return int(v)
        elif dtype in (&#34;float16&#34;, &#34;float32&#34;, &#34;float64&#34;):
                return float(v)
        elif dtype == &#34;bool&#34;:
                return bool(v)
        elif dtype == &#34;string&#34;:
                return str(v)
        else:
                raise TypeError()


@fdb.transactional
def write_tensor_values(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, name: str, tensor: shoji.Tensor) -&gt; None:
        CHUNK_SIZE = 1_000
        subspace = wsm._subspace
        if tensor.inits is not None:
                codec = shoji.Codec(tensor.dtype)
                if np.ndim(tensor.inits) == 0:  # It&#39;s a scalar value
                        key = subspace.pack((&#34;tensor_values&#34;, name) + (0, 0))
                        tr[key] = codec.encode(np.array(tensor.inits))
                else:
                        # Update the length
                        old_length: int = wsm[name].length  # type: ignore
                        length = len(tensor.inits) + old_length  # type: ignore
                        tr.clear_range_startswith(subspace[&#34;tensors&#34;][name].key())
                        key_tuple = (&#34;tensors&#34;, name, tensor.dtype, tensor.rank) + tensor.dims + (1 if tensor.jagged else 0, length)
                        key = subspace.pack(key_tuple)
                        tr[key] = b&#39;&#39;
                        if tensor.rank == 1:
                                # Save the values unchunked
                                for i in range(len(tensor.inits)):
                                        key = subspace.pack((&#34;tensor_values&#34;, name, i + old_length, 0))
                                        x = tensor.inits[i]
                                        if tensor.dtype == &#34;string&#34;:
                                                tr[key] = x.encode()
                                        elif tensor.dtype == &#34;float32&#34;:
                                                tr[key] = struct.pack(&#34;f&#34;, x)
                                        elif tensor.dtype == &#34;float64&#34;:
                                                tr[key] = struct.pack(&#34;d&#34;, x)
                                        elif tensor.dtype == &#34;uint16&#34;:
                                                tr[key] = int(x).to_bytes(2, &#34;little&#34;, signed=False)
                                        elif tensor.dtype == &#34;uint32&#34;:
                                                tr[key] = int(x).to_bytes(4, &#34;little&#34;, signed=False)
                                        elif tensor.dtype == &#34;uint64&#34;:
                                                tr[key] = int(x).to_bytes(8, &#34;little&#34;, signed=False)
                                        elif tensor.dtype == &#34;int16&#34;:
                                                tr[key] = int(x).to_bytes(2, &#34;little&#34;, signed=True)
                                        elif tensor.dtype == &#34;int32&#34;:
                                                tr[key] = int(x).to_bytes(4, &#34;little&#34;, signed=True)
                                        elif tensor.dtype == &#34;int64&#34;:
                                                tr[key] = int(x).to_bytes(8, &#34;little&#34;, signed=True)
                                        elif tensor.dtype == &#34;bool&#34;:
                                                tr[key] = int(x).to_bytes(1, &#34;little&#34;, signed=False)

                                # Create an index
                                values = [coerce_dtype(tensor.dtype, v) for v in tensor.inits]
                                for i, value in enumerate(values):
                                        key = subspace.pack((&#34;tensor_indexes&#34;, name, value, int(i + old_length)))
                                        tr[key] = b&#39;&#39;
                        else:
                                for i in range(len(tensor.inits)):
                                        encoded = codec.encode(np.array(tensor.inits[i]))
                                        for j in range(0, len(encoded), CHUNK_SIZE):
                                                key = subspace.pack((&#34;tensor_values&#34;, name, i + old_length, j // CHUNK_SIZE))
                                                tr[key] = encoded[j:j+CHUNK_SIZE]


@fdb.transactional
def update_tensor_values(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, name: str, indices: Tuple[slice, np.ndarray], tensor: shoji.Tensor) -&gt; None:
        CHUNK_SIZE = 1_000
        subspace = wsm._subspace
        codec = shoji.Codec(tensor.dtype)
        # Better read the tensor metadata again inside the transaction
        temp = tensor.inits
        tensor: shoji.Tensor = wsm[name]
        tensor.inits = temp

        if isinstance(indices[0], slice):
                s = indices[0].indices(tensor.length)
                rows = np.arange(s[0], s[1], s[2])
        else:
                rows = indices[0]

        if np.ndim(tensor.inits) == 0:  # It&#39;s a scalar value
                assert len(indices) != 0, &#34;Cannot index scalar value&#34;
                key = subspace.pack((&#34;tensor_values&#34;, name) + (0, 0))
                tr[key] = codec.encode(np.array(tensor.inits))
        else:
                # Read the current data that we want to modify
                if isinstance(indices[0], slice):
                        prev = read_filtered_tensor(tr, subspace, name, tensor, np.arange(indices[0].start, indices[0].stop, indices[0].step))
                else:
                        prev = read_filtered_tensor(tr, subspace, name, tensor, indices[0])

                # Modify the data row by row (in case the tensor is jagged)
                for ix in range(len(prev)):
                        row = prev[ix]
                        
                        if tensor.rank == 1:
                                # Remove index entries
                                key = subspace.pack((&#34;tensor_indexes&#34;, name, coerce_dtype(tensor.dtype, row), int(rows[ix])))
                                del tr[key]
                                prev[ix] = tensor.inits[ix]
                        else:
                                # Expand any slices in the indices
                                actual_indices = []
                                for i, ind in enumerate(indices[1:]):
                                        if isinstance(ind, slice):
                                                s = ind.indices(prev.shape[i + 1])
                                                actual_indices.append(np.arange(s[0], s[1], s[2]))
                                        else:
                                                actual_indices.append(ind)
                                row[actual_indices] = tensor.inits

                # Now save the tensor back to the appropriate rows
                if tensor.rank == 1:
                        for i, j in enumerate(rows):
                                key = subspace.pack((&#34;tensor_values&#34;, name, int(j), 0))
                                x = tensor.inits[i]
                                if tensor.dtype == &#34;string&#34;:
                                        tr[key] = x.encode()
                                elif tensor.dtype == &#34;float32&#34;:
                                        tr[key] = struct.pack(&#34;f&#34;, x)
                                elif tensor.dtype == &#34;float64&#34;:
                                        tr[key] = struct.pack(&#34;d&#34;, x)
                                elif tensor.dtype == &#34;uint16&#34;:
                                        tr[key] = int(x).to_bytes(2, &#34;little&#34;, signed=False)
                                elif tensor.dtype == &#34;uint32&#34;:
                                        tr[key] = int(x).to_bytes(4, &#34;little&#34;, signed=False)
                                elif tensor.dtype == &#34;uint64&#34;:
                                        tr[key] = int(x).to_bytes(8, &#34;little&#34;, signed=False)
                                elif tensor.dtype == &#34;int16&#34;:
                                        tr[key] = int(x).to_bytes(2, &#34;little&#34;, signed=True)
                                elif tensor.dtype == &#34;int32&#34;:
                                        tr[key] = int(x).to_bytes(4, &#34;little&#34;, signed=True)
                                elif tensor.dtype == &#34;int64&#34;:
                                        tr[key] = int(x).to_bytes(8, &#34;little&#34;, signed=True)
                                elif tensor.dtype == &#34;bool&#34;:
                                        tr[key] = int(x).to_bytes(1, &#34;little&#34;, signed=False)

                        # Create index entries
                        values = [coerce_dtype(tensor.dtype, v) for v in tensor.inits]
                        for i, value in enumerate(values):
                                key = subspace.pack((&#34;tensor_indexes&#34;, name, value, int(rows[i])))
                                tr[key] = b&#39;&#39;
                else:
                        for i, j in enumerate(rows):
                                encoded = codec.encode(np.array(tensor.inits[i]))
                                for k in range(0, len(encoded), CHUNK_SIZE):
                                        key = subspace.pack((&#34;tensor_values&#34;, name, int(j), k // CHUNK_SIZE))
                                        tr[key] = encoded[k:k + CHUNK_SIZE]


@numba.jit
def compute_ranges(elements):
    elements = np.sort(elements)
    ranges = []
    start = elements[0]
    for ix in range(1, len(elements)):
        if elements[ix] != elements[ix - 1] + 1:
            ranges.append((start, elements[ix - 1]))
            start = elements[ix]
    ranges.append((start, elements[-1]))
    return ranges


@fdb.transactional
def read_chunked_rows(tr: fdb.impl.Transaction, subspace: fdb.subspace_impl.Subspace, name: str, i: int, j: int, codec: shoji.Codec) -&gt; List[np.ndarray]:
        start = subspace.range((&#34;tensor_values&#34;, name, i)).start
        stop = subspace.range((&#34;tensor_values&#34;, name, j)).stop
        result = []
        ix = i
        encoded = bytearray()
        for k, v in tr.get_range(start, stop):
                row = subspace.unpack(k)[-2]
                if row != ix:
                        result.append(codec.decode(bytes(encoded)))
                        encoded = bytearray()
                        ix = row
                encoded += v
        result.append(codec.decode(bytes(encoded)))
        return result

@fdb.transactional
def read_unchunked_rows(tr: fdb.impl.Transaction, subspace: fdb.subspace_impl.Subspace, name: str, i: int, j: int, dtype: str) -&gt; Union[List[float], List[int], List[str], List[bool]]:
        start = subspace.range((&#34;tensor_values&#34;, name, i)).start
        stop = subspace.range((&#34;tensor_values&#34;, name, j)).stop
        vals = [v for _,v in tr.get_range(start, stop)]
        if dtype == &#34;string&#34;:
                return [x.decode() for x in vals]
        if dtype == &#34;float32&#34;:
                return [struct.unpack(&#34;f&#34;, x)[0] for x in vals]
        if dtype == &#34;float64&#34;:
                return [struct.unpack(&#34;d&#34;, x)[0] for x in vals]
        if dtype == &#34;bool&#34;:
                return [bool(int.from_bytes(x, &#34;little&#34;, signed=False)) for x in vals]
        signed = True
        if dtype[0] == &#34;u&#34;:
                signed = False
        return [int.from_bytes(x, &#34;little&#34;, signed=signed) for x in vals]


@fdb.transactional
def read_filtered_tensor(tr: fdb.impl.Transaction, subspace: fdb.subspace_impl.Subspace, name: str, tensor: shoji.Tensor, indices: np.ndarray = None) -&gt; np.ndarray:
        assert tensor.length is not None
        # Convert the list of indices to ranges as far as possible
        if indices is None:
                ranges = [(0, tensor.length)]
                n_rows = tensor.length
        else:
                ranges = compute_ranges(indices)
                n_rows = len(indices)
        codec = shoji.Codec(tensor.dtype)

        if tensor.jagged:
                resultj: List[np.ndarray] = []
                for (start, stop) in ranges:
                        resultj += read_chunked_rows(tr, subspace, name, start, stop, codec)
                return resultj
        else:
                if tensor.rank == 1:
                        result = np.empty(n_rows, dtype=tensor.numpy_dtype())
                        ix = 0
                        for (start, stop) in ranges:
                                rows = read_unchunked_rows(tr, subspace, name, start, stop, dtype=tensor.dtype)
                                result[ix: ix + (stop - start + 1)] = rows
                                ix += stop - start + 1
                else:
                        result = None
                        ix = 0
                        for start, stop in ranges:
                                rows = read_chunked_rows(tr, subspace, name, start, stop, codec)
                                if result is None:
                                        result = np.empty((n_rows,) + rows[0].shape, dtype=rows[0].dtype)
                                result[ix: ix + (stop - start + 1)] = np.array(rows)
                                ix += (stop - start + 1)
        return result


@fdb.transactional
def const_compare(tr, wsm: shoji.WorkspaceManager, name: str, operator: str, const: Tuple[int, str, float]) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Compare a tensor to a constant value, and return all indices that match
        &#34;&#34;&#34;
        # Code for range, equality and inequality filters
        tensor = wsm[name]
        assert isinstance(tensor, shoji.Tensor)
        const = tensor.python_dtype()(const)  # Cast the const to string, float or int
        index = wsm._subspace[&#34;tensor_indexes&#34;][name]
        eq_range = index[const].range()
        all_range = index.range()
        start, stop = all_range.start, all_range.stop
        if operator == &#34;==&#34;:
                start, stop = eq_range.start, eq_range.stop
        if operator == &#34;&gt;=&#34;:
                start = eq_range.start
        elif operator == &#34;&gt;&#34;:
                start = tr.get_key(fdb.KeySelector.first_greater_than(index[const]))
        elif operator == &#34;&lt;=&#34;:
                stop = eq_range.stop
        elif operator == &#34;&lt;&#34;:
                stop = tr.get_key(fdb.KeySelector.last_less_than(index[const]))
        return np.array([index.unpack(k)[1] for k, _ in tr[start:stop]], dtype=&#34;int64&#34;)


@fdb.transactional
def append_tensors(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, dname: str, vals: Dict[str, Union[List[np.ndarray], np.ndarray]]) -&gt; None:
        &#34;&#34;&#34;
        Append values to a set of named tensors, which share their first dimension

        Args:
                tr: Transaction
                wsm: `shoji.workspace.WorkspaceManager`
                dname: Name of the dimension along which the values should be appended
                vals: dict of tensor names and corresponding values (np.ndarray or list of np.ndarray)

        Remarks:
                The function will check for the existence of all the tensors, that they have the same first dimension,
                and that no other tensor in the workspace has the same first dimension. It will also check that the values
                given have the same length along the first dimension, and that all other dimensions of each tensor match the 
                definitions of those tensors.
        &#34;&#34;&#34;
        subspace = wsm._subspace
        tensors: Dict[str, shoji.Tensor] = {}

        # Check that all named tensors exist, and have the right first dimension
        for name in vals.keys():
                if name_exists(tr, wsm, name) != &#34;Tensor&#34;:
                        raise NameError(f&#34;Tensor &#39;{name}&#39; does not exist in the workspace&#34;)
                tensors[name] = wsm[name]  # type: ignore
                if tensors[name].rank == 0 or tensors[name].dims[0] != dname:
                        raise ValueError(f&#34;Tensor &#39;{name}&#39; does not have &#39;{dname}&#39; as first dimension&#34;)

        # Check that all relevant tensors have been included in the values
        all_tensors: List[str] = [subspace[&#34;tensors&#34;].unpack(k)[0] for k,v in tr[subspace[&#34;tensors&#34;].range()]]
        for tensor_name in all_tensors:
                if tensor_name not in vals:
                        tensor: shoji.Tensor = wsm[tensor_name]  # type: ignore
                        if tensor.rank != 0 and tensor.dims[0] == dname:
                                raise ValueError(f&#34;Tensor &#39;{tensor.assigned_name}&#39; missing from values in append operation&#34;)

        # Check the rank of the values
        for name, values in vals.items():
                if tensors[name].jagged:
                        for row in values:
                                if tensors[name].rank != row.ndim + 1:  # type: ignore
                                        raise ValueError(f&#34;Tensor &#39;{name}&#39; of rank {tensors[name].rank} cannot be initialized with rank-{row.ndim + 1} array&#34;)  # type: ignore
                else:
                        if tensors[name].rank != values.ndim:  # type: ignore
                                raise ValueError(f&#34;Tensor &#39;{name}&#39; of rank {tensors[name].rank} cannot be initialized with rank-{values.ndim} array&#34;)  # type: ignore

        # Check that all the other dimensions are the correct shape according to their definitions
        for name, values in vals.items():
                for i, idim in enumerate(tensors[name].dims):
                        if i == 0 or idim is None:
                                continue
                        elif isinstance(idim, int):  # Anonymous fixed-shape dimension
                                pass  # This constraint was already checked when the tensor was created
                        elif isinstance(idim, str):  # Named dimension
                                dim: shoji.Dimension = wsm[idim]  # type: ignore
                                if dim.shape is None:
                                        continue
                                if tensors[name].jagged:
                                        for row in values:
                                                if row.shape[i] != dim.shape: 
                                                        raise ValueError(f&#34;Tensor &#39;{name}&#39; dimension &#39;{idim}&#39; must be exactly {dim.shape} elements long&#34;)
                                elif dim.shape != values.shape[i]:  # type: ignore
                                        raise ValueError(f&#34;Tensor &#39;{name}&#39; dimension {i} (&#39;{idim}&#39;) must be exactly {dim.shape} elements long&#34;)
                                
        # Write the values
        for name, values in vals.items():
                tensors[name].inits = values
                write_tensor_values(tr, wsm, name, tensors[name])

        # Update the first dimension
        dim = wsm[dname]  # type: ignore
        dim.length = dim.length + len(values)
        create_or_update_dimension(tr, wsm, dname, dim)


def __nil__():
        pass</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="shoji.io.append_tensors"><code class="name flex">
<span>def <span class="ident">append_tensors</span></span>(<span>tr: fdb.impl.Transaction, wsm: <a title="shoji.workspace.WorkspaceManager" href="workspace.html#shoji.workspace.WorkspaceManager">WorkspaceManager</a>, dname: str, vals: Dict[str, Union[List[numpy.ndarray], numpy.ndarray]]) -> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Append values to a set of named tensors, which share their first dimension</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tr</code></strong></dt>
<dd>Transaction</dd>
<dt><strong><code>wsm</code></strong></dt>
<dd><code><a title="shoji.workspace.WorkspaceManager" href="workspace.html#shoji.workspace.WorkspaceManager">WorkspaceManager</a></code></dd>
<dt><strong><code>dname</code></strong></dt>
<dd>Name of the dimension along which the values should be appended</dd>
<dt><strong><code>vals</code></strong></dt>
<dd>dict of tensor names and corresponding values (np.ndarray or list of np.ndarray)</dd>
</dl>
<h2 id="remarks">Remarks</h2>
<p>The function will check for the existence of all the tensors, that they have the same first dimension,
and that no other tensor in the workspace has the same first dimension. It will also check that the values
given have the same length along the first dimension, and that all other dimensions of each tensor match the
definitions of those tensors.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@fdb.transactional
def append_tensors(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, dname: str, vals: Dict[str, Union[List[np.ndarray], np.ndarray]]) -&gt; None:
        &#34;&#34;&#34;
        Append values to a set of named tensors, which share their first dimension

        Args:
                tr: Transaction
                wsm: `shoji.workspace.WorkspaceManager`
                dname: Name of the dimension along which the values should be appended
                vals: dict of tensor names and corresponding values (np.ndarray or list of np.ndarray)

        Remarks:
                The function will check for the existence of all the tensors, that they have the same first dimension,
                and that no other tensor in the workspace has the same first dimension. It will also check that the values
                given have the same length along the first dimension, and that all other dimensions of each tensor match the 
                definitions of those tensors.
        &#34;&#34;&#34;
        subspace = wsm._subspace
        tensors: Dict[str, shoji.Tensor] = {}

        # Check that all named tensors exist, and have the right first dimension
        for name in vals.keys():
                if name_exists(tr, wsm, name) != &#34;Tensor&#34;:
                        raise NameError(f&#34;Tensor &#39;{name}&#39; does not exist in the workspace&#34;)
                tensors[name] = wsm[name]  # type: ignore
                if tensors[name].rank == 0 or tensors[name].dims[0] != dname:
                        raise ValueError(f&#34;Tensor &#39;{name}&#39; does not have &#39;{dname}&#39; as first dimension&#34;)

        # Check that all relevant tensors have been included in the values
        all_tensors: List[str] = [subspace[&#34;tensors&#34;].unpack(k)[0] for k,v in tr[subspace[&#34;tensors&#34;].range()]]
        for tensor_name in all_tensors:
                if tensor_name not in vals:
                        tensor: shoji.Tensor = wsm[tensor_name]  # type: ignore
                        if tensor.rank != 0 and tensor.dims[0] == dname:
                                raise ValueError(f&#34;Tensor &#39;{tensor.assigned_name}&#39; missing from values in append operation&#34;)

        # Check the rank of the values
        for name, values in vals.items():
                if tensors[name].jagged:
                        for row in values:
                                if tensors[name].rank != row.ndim + 1:  # type: ignore
                                        raise ValueError(f&#34;Tensor &#39;{name}&#39; of rank {tensors[name].rank} cannot be initialized with rank-{row.ndim + 1} array&#34;)  # type: ignore
                else:
                        if tensors[name].rank != values.ndim:  # type: ignore
                                raise ValueError(f&#34;Tensor &#39;{name}&#39; of rank {tensors[name].rank} cannot be initialized with rank-{values.ndim} array&#34;)  # type: ignore

        # Check that all the other dimensions are the correct shape according to their definitions
        for name, values in vals.items():
                for i, idim in enumerate(tensors[name].dims):
                        if i == 0 or idim is None:
                                continue
                        elif isinstance(idim, int):  # Anonymous fixed-shape dimension
                                pass  # This constraint was already checked when the tensor was created
                        elif isinstance(idim, str):  # Named dimension
                                dim: shoji.Dimension = wsm[idim]  # type: ignore
                                if dim.shape is None:
                                        continue
                                if tensors[name].jagged:
                                        for row in values:
                                                if row.shape[i] != dim.shape: 
                                                        raise ValueError(f&#34;Tensor &#39;{name}&#39; dimension &#39;{idim}&#39; must be exactly {dim.shape} elements long&#34;)
                                elif dim.shape != values.shape[i]:  # type: ignore
                                        raise ValueError(f&#34;Tensor &#39;{name}&#39; dimension {i} (&#39;{idim}&#39;) must be exactly {dim.shape} elements long&#34;)
                                
        # Write the values
        for name, values in vals.items():
                tensors[name].inits = values
                write_tensor_values(tr, wsm, name, tensors[name])

        # Update the first dimension
        dim = wsm[dname]  # type: ignore
        dim.length = dim.length + len(values)
        create_or_update_dimension(tr, wsm, dname, dim)</code></pre>
</details>
</dd>
<dt id="shoji.io.coerce_dtype"><code class="name flex">
<span>def <span class="ident">coerce_dtype</span></span>(<span>dtype, v) -> Union[int, float, bool, str]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def coerce_dtype(dtype, v) -&gt; Union[int, float, bool, str]:
        if dtype in (&#34;uint8&#34;, &#34;uint16&#34;, &#34;uint32&#34;, &#34;uint64&#34;, &#34;int8&#34;, &#34;int16&#34;, &#34;int32&#34;, &#34;int64&#34;):
                return int(v)
        elif dtype in (&#34;float16&#34;, &#34;float32&#34;, &#34;float64&#34;):
                return float(v)
        elif dtype == &#34;bool&#34;:
                return bool(v)
        elif dtype == &#34;string&#34;:
                return str(v)
        else:
                raise TypeError()</code></pre>
</details>
</dd>
<dt id="shoji.io.compute_ranges"><code class="name flex">
<span>def <span class="ident">compute_ranges</span></span>(<span>elements)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@numba.jit
def compute_ranges(elements):
    elements = np.sort(elements)
    ranges = []
    start = elements[0]
    for ix in range(1, len(elements)):
        if elements[ix] != elements[ix - 1] + 1:
            ranges.append((start, elements[ix - 1]))
            start = elements[ix]
    ranges.append((start, elements[-1]))
    return ranges</code></pre>
</details>
</dd>
<dt id="shoji.io.const_compare"><code class="name flex">
<span>def <span class="ident">const_compare</span></span>(<span>tr, wsm: <a title="shoji.workspace.WorkspaceManager" href="workspace.html#shoji.workspace.WorkspaceManager">WorkspaceManager</a>, name: str, operator: str, const: Tuple[int, str, float]) -> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Compare a tensor to a constant value, and return all indices that match</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@fdb.transactional
def const_compare(tr, wsm: shoji.WorkspaceManager, name: str, operator: str, const: Tuple[int, str, float]) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Compare a tensor to a constant value, and return all indices that match
        &#34;&#34;&#34;
        # Code for range, equality and inequality filters
        tensor = wsm[name]
        assert isinstance(tensor, shoji.Tensor)
        const = tensor.python_dtype()(const)  # Cast the const to string, float or int
        index = wsm._subspace[&#34;tensor_indexes&#34;][name]
        eq_range = index[const].range()
        all_range = index.range()
        start, stop = all_range.start, all_range.stop
        if operator == &#34;==&#34;:
                start, stop = eq_range.start, eq_range.stop
        if operator == &#34;&gt;=&#34;:
                start = eq_range.start
        elif operator == &#34;&gt;&#34;:
                start = tr.get_key(fdb.KeySelector.first_greater_than(index[const]))
        elif operator == &#34;&lt;=&#34;:
                stop = eq_range.stop
        elif operator == &#34;&lt;&#34;:
                stop = tr.get_key(fdb.KeySelector.last_less_than(index[const]))
        return np.array([index.unpack(k)[1] for k, _ in tr[start:stop]], dtype=&#34;int64&#34;)</code></pre>
</details>
</dd>
<dt id="shoji.io.create_or_update_dimension"><code class="name flex">
<span>def <span class="ident">create_or_update_dimension</span></span>(<span>tr, wsm: <a title="shoji.workspace.WorkspaceManager" href="workspace.html#shoji.workspace.WorkspaceManager">WorkspaceManager</a>, name: str, dim: <a title="shoji.dimension.Dimension" href="dimension.html#shoji.dimension.Dimension">Dimension</a>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@fdb.transactional
def create_or_update_dimension(tr, wsm: shoji.WorkspaceManager, name: str, dim: shoji.Dimension):
        subspace = wsm._subspace
        # Check that name doesn&#39;t already exist
        existing_name = name_exists(tr, wsm, name)
        if existing_name is not None:
                if existing_name != &#34;Dimension&#34;:
                        raise AttributeError(f&#34;Name already exists (as {existing_name})&#34;)
                # Update an existing dimension
                prev_dim: shoji.Dimension = read_entity(tr, wsm, name)
                if prev_dim.shape != dim.shape:
                        raise AttributeError(f&#34;Cannot modify shape of existing dimension &#39;{name}&#39;&#34;)
        # Create or update the dimension
        tr[subspace[&#34;dimensions&#34;][name]] = (dim.shape if dim.shape is not None else -1).to_bytes(8, &#34;little&#34;, signed=True) + dim.length.to_bytes(8, &#34;little&#34;, signed=False)</code></pre>
</details>
</dd>
<dt id="shoji.io.create_tensor"><code class="name flex">
<span>def <span class="ident">create_tensor</span></span>(<span>tr: fdb.impl.Transaction, wsm: <a title="shoji.workspace.WorkspaceManager" href="workspace.html#shoji.workspace.WorkspaceManager">WorkspaceManager</a>, name: str, tensor: <a title="shoji.tensor.Tensor" href="tensor.html#shoji.tensor.Tensor">Tensor</a>) -> NoneType</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@fdb.transactional
def create_tensor(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, name: str, tensor: shoji.Tensor) -&gt; None:
        subspace = wsm._subspace
        # Check that name doesn&#39;t already exist
        existing_name = name_exists(tr, wsm, name)
        if existing_name is not None:
                raise AttributeError(f&#34;Name already exists (as {existing_name})&#34;)
        # Check that the dimensions of the tensor exist
        for ix, d in enumerate(tensor.dims):
                if isinstance(d, str):
                        dim = wsm[d]
                        assert isinstance(dim, shoji.Dimension)
                        # Check that the dimensions of the tensor match the shape of the tensor
                        if dim.shape is None:
                                if ix &gt; 0:
                                        tensor.jagged = True
                        else:
                                if tensor.inits is not None and tensor.shape[ix] != dim.shape:
                                        raise IndexError(f&#34;Mismatch between the declared shape {dim.shape} of dimension &#39;{d}&#39; and the shape {tensor.shape} of values&#34;)
        # Store tensor definition
        # (&#34;tensors&#34;, name, dtype, rank) + dims + (jagged, length)
        key_tuple = (&#34;tensors&#34;, name, tensor.dtype, tensor.rank) + tensor.dims + (1 if tensor.jagged else 0, 0)
        key = subspace.pack(key_tuple)
        tr[key] = b&#39;&#39;</code></pre>
</details>
</dd>
<dt id="shoji.io.delete_entity"><code class="name flex">
<span>def <span class="ident">delete_entity</span></span>(<span>tr: fdb.impl.Transaction, wsm: <a title="shoji.workspace.WorkspaceManager" href="workspace.html#shoji.workspace.WorkspaceManager">WorkspaceManager</a>, name: str) -> NoneType</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@fdb.transactional
def delete_entity(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, name: str) -&gt; None:
        subspace = wsm._subspace
        if subspace.exists(tr, name):
                subspace.open(tr, name).remove(tr)
        elif tr[subspace[&#34;dimensions&#34;][name]].present():
                tr.clear_range_startswith(subspace[&#34;dimensions&#34;][name].key())
        elif tr[subspace[&#34;tensors&#34;][name]].present():
                tr.clear_range_startswith(subspace[&#34;tensors&#34;][name].key())
                tr.clear_range_startswith(subspace[&#34;tensor_values&#34;][name].key())</code></pre>
</details>
</dd>
<dt id="shoji.io.name_exists"><code class="name flex">
<span>def <span class="ident">name_exists</span></span>(<span>tr: fdb.impl.Transaction, wsm: <a title="shoji.workspace.WorkspaceManager" href="workspace.html#shoji.workspace.WorkspaceManager">WorkspaceManager</a>, name: str) -> Union[str, NoneType]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@fdb.transactional
def name_exists(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, name: str) -&gt; Optional[str]:
        subspace = wsm._subspace
        if subspace.exists(tr, wsm._path + (name,)):
                return &#34;Workspace&#34;
        if tr[subspace[&#34;dimensions&#34;][name]].present():
                return &#34;Dimension&#34;
        for _ in tr[subspace.range((&#34;tensors&#34;, name))]:
                return &#34;Tensor&#34;
        return None</code></pre>
</details>
</dd>
<dt id="shoji.io.read_chunked_rows"><code class="name flex">
<span>def <span class="ident">read_chunked_rows</span></span>(<span>tr: fdb.impl.Transaction, subspace: fdb.subspace_impl.Subspace, name: str, i: int, j: int, codec: <a title="shoji.codec.Codec" href="codec.html#shoji.codec.Codec">Codec</a>) -> List[numpy.ndarray]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@fdb.transactional
def read_chunked_rows(tr: fdb.impl.Transaction, subspace: fdb.subspace_impl.Subspace, name: str, i: int, j: int, codec: shoji.Codec) -&gt; List[np.ndarray]:
        start = subspace.range((&#34;tensor_values&#34;, name, i)).start
        stop = subspace.range((&#34;tensor_values&#34;, name, j)).stop
        result = []
        ix = i
        encoded = bytearray()
        for k, v in tr.get_range(start, stop):
                row = subspace.unpack(k)[-2]
                if row != ix:
                        result.append(codec.decode(bytes(encoded)))
                        encoded = bytearray()
                        ix = row
                encoded += v
        result.append(codec.decode(bytes(encoded)))
        return result</code></pre>
</details>
</dd>
<dt id="shoji.io.read_entity"><code class="name flex">
<span>def <span class="ident">read_entity</span></span>(<span>tr: fdb.impl.Transaction, wsm: <a title="shoji.workspace.WorkspaceManager" href="workspace.html#shoji.workspace.WorkspaceManager">WorkspaceManager</a>, name: str) -> Union[<a title="shoji.dimension.Dimension" href="dimension.html#shoji.dimension.Dimension">Dimension</a>, <a title="shoji.workspace.WorkspaceManager" href="workspace.html#shoji.workspace.WorkspaceManager">WorkspaceManager</a>, <a title="shoji.tensor.Tensor" href="tensor.html#shoji.tensor.Tensor">Tensor</a>, NoneType]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@fdb.transactional
def read_entity(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, name: str) -&gt; Union[shoji.Dimension, shoji.WorkspaceManager, shoji.Tensor, None]:
        subspace = wsm._subspace
        if subspace.exists(tr, name):
                child = subspace.open(tr, name)
                return shoji.WorkspaceManager(wsm._db, child, wsm._path + (name,))
        elif tr[subspace[&#34;dimensions&#34;][name]].present():
                val = tr[subspace[&#34;dimensions&#34;][name]]
                dim = shoji.Dimension(shape=int.from_bytes(val[:8], &#34;little&#34;, signed=True), length=int.from_bytes(val[8:], &#34;little&#34;, signed=False))
                dim.assigned_name = name
                dim.wsm = wsm
                return dim
        else:
                # key_tuple = (&#34;tensors&#34;, name, tensor.dtype, tensor.rank) + tensor.dims + (jagged, length)
                tensor_tuples = tr[subspace.range((&#34;tensors&#34;, name))]
                for k, _ in tensor_tuples:
                        key = subspace.unpack(k)
                        tensor = shoji.Tensor(key[2], key[4:4+key[3]], length=key[-1])
                        tensor.jagged = key[-2] == 1
                        tensor.assigned_name = name
                        tensor.wsm = wsm
                        return tensor
        raise KeyError(f&#34;{name} not found&#34;)</code></pre>
</details>
</dd>
<dt id="shoji.io.read_filtered_tensor"><code class="name flex">
<span>def <span class="ident">read_filtered_tensor</span></span>(<span>tr: fdb.impl.Transaction, subspace: fdb.subspace_impl.Subspace, name: str, tensor: <a title="shoji.tensor.Tensor" href="tensor.html#shoji.tensor.Tensor">Tensor</a>, indices: numpy.ndarray = None) -> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@fdb.transactional
def read_filtered_tensor(tr: fdb.impl.Transaction, subspace: fdb.subspace_impl.Subspace, name: str, tensor: shoji.Tensor, indices: np.ndarray = None) -&gt; np.ndarray:
        assert tensor.length is not None
        # Convert the list of indices to ranges as far as possible
        if indices is None:
                ranges = [(0, tensor.length)]
                n_rows = tensor.length
        else:
                ranges = compute_ranges(indices)
                n_rows = len(indices)
        codec = shoji.Codec(tensor.dtype)

        if tensor.jagged:
                resultj: List[np.ndarray] = []
                for (start, stop) in ranges:
                        resultj += read_chunked_rows(tr, subspace, name, start, stop, codec)
                return resultj
        else:
                if tensor.rank == 1:
                        result = np.empty(n_rows, dtype=tensor.numpy_dtype())
                        ix = 0
                        for (start, stop) in ranges:
                                rows = read_unchunked_rows(tr, subspace, name, start, stop, dtype=tensor.dtype)
                                result[ix: ix + (stop - start + 1)] = rows
                                ix += stop - start + 1
                else:
                        result = None
                        ix = 0
                        for start, stop in ranges:
                                rows = read_chunked_rows(tr, subspace, name, start, stop, codec)
                                if result is None:
                                        result = np.empty((n_rows,) + rows[0].shape, dtype=rows[0].dtype)
                                result[ix: ix + (stop - start + 1)] = np.array(rows)
                                ix += (stop - start + 1)
        return result</code></pre>
</details>
</dd>
<dt id="shoji.io.read_unchunked_rows"><code class="name flex">
<span>def <span class="ident">read_unchunked_rows</span></span>(<span>tr: fdb.impl.Transaction, subspace: fdb.subspace_impl.Subspace, name: str, i: int, j: int, dtype: str) -> Union[List[float], List[int], List[str], List[bool]]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@fdb.transactional
def read_unchunked_rows(tr: fdb.impl.Transaction, subspace: fdb.subspace_impl.Subspace, name: str, i: int, j: int, dtype: str) -&gt; Union[List[float], List[int], List[str], List[bool]]:
        start = subspace.range((&#34;tensor_values&#34;, name, i)).start
        stop = subspace.range((&#34;tensor_values&#34;, name, j)).stop
        vals = [v for _,v in tr.get_range(start, stop)]
        if dtype == &#34;string&#34;:
                return [x.decode() for x in vals]
        if dtype == &#34;float32&#34;:
                return [struct.unpack(&#34;f&#34;, x)[0] for x in vals]
        if dtype == &#34;float64&#34;:
                return [struct.unpack(&#34;d&#34;, x)[0] for x in vals]
        if dtype == &#34;bool&#34;:
                return [bool(int.from_bytes(x, &#34;little&#34;, signed=False)) for x in vals]
        signed = True
        if dtype[0] == &#34;u&#34;:
                signed = False
        return [int.from_bytes(x, &#34;little&#34;, signed=signed) for x in vals]</code></pre>
</details>
</dd>
<dt id="shoji.io.update_tensor_values"><code class="name flex">
<span>def <span class="ident">update_tensor_values</span></span>(<span>tr: fdb.impl.Transaction, wsm: <a title="shoji.workspace.WorkspaceManager" href="workspace.html#shoji.workspace.WorkspaceManager">WorkspaceManager</a>, name: str, indices: Tuple[slice, numpy.ndarray], tensor: <a title="shoji.tensor.Tensor" href="tensor.html#shoji.tensor.Tensor">Tensor</a>) -> NoneType</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@fdb.transactional
def update_tensor_values(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, name: str, indices: Tuple[slice, np.ndarray], tensor: shoji.Tensor) -&gt; None:
        CHUNK_SIZE = 1_000
        subspace = wsm._subspace
        codec = shoji.Codec(tensor.dtype)
        # Better read the tensor metadata again inside the transaction
        temp = tensor.inits
        tensor: shoji.Tensor = wsm[name]
        tensor.inits = temp

        if isinstance(indices[0], slice):
                s = indices[0].indices(tensor.length)
                rows = np.arange(s[0], s[1], s[2])
        else:
                rows = indices[0]

        if np.ndim(tensor.inits) == 0:  # It&#39;s a scalar value
                assert len(indices) != 0, &#34;Cannot index scalar value&#34;
                key = subspace.pack((&#34;tensor_values&#34;, name) + (0, 0))
                tr[key] = codec.encode(np.array(tensor.inits))
        else:
                # Read the current data that we want to modify
                if isinstance(indices[0], slice):
                        prev = read_filtered_tensor(tr, subspace, name, tensor, np.arange(indices[0].start, indices[0].stop, indices[0].step))
                else:
                        prev = read_filtered_tensor(tr, subspace, name, tensor, indices[0])

                # Modify the data row by row (in case the tensor is jagged)
                for ix in range(len(prev)):
                        row = prev[ix]
                        
                        if tensor.rank == 1:
                                # Remove index entries
                                key = subspace.pack((&#34;tensor_indexes&#34;, name, coerce_dtype(tensor.dtype, row), int(rows[ix])))
                                del tr[key]
                                prev[ix] = tensor.inits[ix]
                        else:
                                # Expand any slices in the indices
                                actual_indices = []
                                for i, ind in enumerate(indices[1:]):
                                        if isinstance(ind, slice):
                                                s = ind.indices(prev.shape[i + 1])
                                                actual_indices.append(np.arange(s[0], s[1], s[2]))
                                        else:
                                                actual_indices.append(ind)
                                row[actual_indices] = tensor.inits

                # Now save the tensor back to the appropriate rows
                if tensor.rank == 1:
                        for i, j in enumerate(rows):
                                key = subspace.pack((&#34;tensor_values&#34;, name, int(j), 0))
                                x = tensor.inits[i]
                                if tensor.dtype == &#34;string&#34;:
                                        tr[key] = x.encode()
                                elif tensor.dtype == &#34;float32&#34;:
                                        tr[key] = struct.pack(&#34;f&#34;, x)
                                elif tensor.dtype == &#34;float64&#34;:
                                        tr[key] = struct.pack(&#34;d&#34;, x)
                                elif tensor.dtype == &#34;uint16&#34;:
                                        tr[key] = int(x).to_bytes(2, &#34;little&#34;, signed=False)
                                elif tensor.dtype == &#34;uint32&#34;:
                                        tr[key] = int(x).to_bytes(4, &#34;little&#34;, signed=False)
                                elif tensor.dtype == &#34;uint64&#34;:
                                        tr[key] = int(x).to_bytes(8, &#34;little&#34;, signed=False)
                                elif tensor.dtype == &#34;int16&#34;:
                                        tr[key] = int(x).to_bytes(2, &#34;little&#34;, signed=True)
                                elif tensor.dtype == &#34;int32&#34;:
                                        tr[key] = int(x).to_bytes(4, &#34;little&#34;, signed=True)
                                elif tensor.dtype == &#34;int64&#34;:
                                        tr[key] = int(x).to_bytes(8, &#34;little&#34;, signed=True)
                                elif tensor.dtype == &#34;bool&#34;:
                                        tr[key] = int(x).to_bytes(1, &#34;little&#34;, signed=False)

                        # Create index entries
                        values = [coerce_dtype(tensor.dtype, v) for v in tensor.inits]
                        for i, value in enumerate(values):
                                key = subspace.pack((&#34;tensor_indexes&#34;, name, value, int(rows[i])))
                                tr[key] = b&#39;&#39;
                else:
                        for i, j in enumerate(rows):
                                encoded = codec.encode(np.array(tensor.inits[i]))
                                for k in range(0, len(encoded), CHUNK_SIZE):
                                        key = subspace.pack((&#34;tensor_values&#34;, name, int(j), k // CHUNK_SIZE))
                                        tr[key] = encoded[k:k + CHUNK_SIZE]</code></pre>
</details>
</dd>
<dt id="shoji.io.write_tensor_values"><code class="name flex">
<span>def <span class="ident">write_tensor_values</span></span>(<span>tr: fdb.impl.Transaction, wsm: <a title="shoji.workspace.WorkspaceManager" href="workspace.html#shoji.workspace.WorkspaceManager">WorkspaceManager</a>, name: str, tensor: <a title="shoji.tensor.Tensor" href="tensor.html#shoji.tensor.Tensor">Tensor</a>) -> NoneType</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@fdb.transactional
def write_tensor_values(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, name: str, tensor: shoji.Tensor) -&gt; None:
        CHUNK_SIZE = 1_000
        subspace = wsm._subspace
        if tensor.inits is not None:
                codec = shoji.Codec(tensor.dtype)
                if np.ndim(tensor.inits) == 0:  # It&#39;s a scalar value
                        key = subspace.pack((&#34;tensor_values&#34;, name) + (0, 0))
                        tr[key] = codec.encode(np.array(tensor.inits))
                else:
                        # Update the length
                        old_length: int = wsm[name].length  # type: ignore
                        length = len(tensor.inits) + old_length  # type: ignore
                        tr.clear_range_startswith(subspace[&#34;tensors&#34;][name].key())
                        key_tuple = (&#34;tensors&#34;, name, tensor.dtype, tensor.rank) + tensor.dims + (1 if tensor.jagged else 0, length)
                        key = subspace.pack(key_tuple)
                        tr[key] = b&#39;&#39;
                        if tensor.rank == 1:
                                # Save the values unchunked
                                for i in range(len(tensor.inits)):
                                        key = subspace.pack((&#34;tensor_values&#34;, name, i + old_length, 0))
                                        x = tensor.inits[i]
                                        if tensor.dtype == &#34;string&#34;:
                                                tr[key] = x.encode()
                                        elif tensor.dtype == &#34;float32&#34;:
                                                tr[key] = struct.pack(&#34;f&#34;, x)
                                        elif tensor.dtype == &#34;float64&#34;:
                                                tr[key] = struct.pack(&#34;d&#34;, x)
                                        elif tensor.dtype == &#34;uint16&#34;:
                                                tr[key] = int(x).to_bytes(2, &#34;little&#34;, signed=False)
                                        elif tensor.dtype == &#34;uint32&#34;:
                                                tr[key] = int(x).to_bytes(4, &#34;little&#34;, signed=False)
                                        elif tensor.dtype == &#34;uint64&#34;:
                                                tr[key] = int(x).to_bytes(8, &#34;little&#34;, signed=False)
                                        elif tensor.dtype == &#34;int16&#34;:
                                                tr[key] = int(x).to_bytes(2, &#34;little&#34;, signed=True)
                                        elif tensor.dtype == &#34;int32&#34;:
                                                tr[key] = int(x).to_bytes(4, &#34;little&#34;, signed=True)
                                        elif tensor.dtype == &#34;int64&#34;:
                                                tr[key] = int(x).to_bytes(8, &#34;little&#34;, signed=True)
                                        elif tensor.dtype == &#34;bool&#34;:
                                                tr[key] = int(x).to_bytes(1, &#34;little&#34;, signed=False)

                                # Create an index
                                values = [coerce_dtype(tensor.dtype, v) for v in tensor.inits]
                                for i, value in enumerate(values):
                                        key = subspace.pack((&#34;tensor_indexes&#34;, name, value, int(i + old_length)))
                                        tr[key] = b&#39;&#39;
                        else:
                                for i in range(len(tensor.inits)):
                                        encoded = codec.encode(np.array(tensor.inits[i]))
                                        for j in range(0, len(encoded), CHUNK_SIZE):
                                                key = subspace.pack((&#34;tensor_values&#34;, name, i + old_length, j // CHUNK_SIZE))
                                                tr[key] = encoded[j:j+CHUNK_SIZE]</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="shoji" href="index.html">shoji</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="shoji.io.append_tensors" href="#shoji.io.append_tensors">append_tensors</a></code></li>
<li><code><a title="shoji.io.coerce_dtype" href="#shoji.io.coerce_dtype">coerce_dtype</a></code></li>
<li><code><a title="shoji.io.compute_ranges" href="#shoji.io.compute_ranges">compute_ranges</a></code></li>
<li><code><a title="shoji.io.const_compare" href="#shoji.io.const_compare">const_compare</a></code></li>
<li><code><a title="shoji.io.create_or_update_dimension" href="#shoji.io.create_or_update_dimension">create_or_update_dimension</a></code></li>
<li><code><a title="shoji.io.create_tensor" href="#shoji.io.create_tensor">create_tensor</a></code></li>
<li><code><a title="shoji.io.delete_entity" href="#shoji.io.delete_entity">delete_entity</a></code></li>
<li><code><a title="shoji.io.name_exists" href="#shoji.io.name_exists">name_exists</a></code></li>
<li><code><a title="shoji.io.read_chunked_rows" href="#shoji.io.read_chunked_rows">read_chunked_rows</a></code></li>
<li><code><a title="shoji.io.read_entity" href="#shoji.io.read_entity">read_entity</a></code></li>
<li><code><a title="shoji.io.read_filtered_tensor" href="#shoji.io.read_filtered_tensor">read_filtered_tensor</a></code></li>
<li><code><a title="shoji.io.read_unchunked_rows" href="#shoji.io.read_unchunked_rows">read_unchunked_rows</a></code></li>
<li><code><a title="shoji.io.update_tensor_values" href="#shoji.io.update_tensor_values">update_tensor_values</a></code></li>
<li><code><a title="shoji.io.write_tensor_values" href="#shoji.io.write_tensor_values">write_tensor_values</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>