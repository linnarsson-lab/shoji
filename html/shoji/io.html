<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.1" />
<title>shoji.io API documentation</title>
<meta name="description" content="Internal low-level I/O routines, not intended for end users." />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>shoji.io</code></h1>
</header>
<section id="section-intro">
<p>Internal low-level I/O routines, not intended for end users.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Internal low-level I/O routines, not intended for end users.
&#34;&#34;&#34;

from typing import Union, Optional, Tuple, Dict, List
import fdb
import numpy as np
import blosc
import shoji
import numba
import numcodecs


@fdb.transactional
def name_exists(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, name: str) -&gt; Optional[str]:
        subspace = wsm._subspace
        if subspace.exists(tr, wsm._path + (name,)):
                return &#34;Workspace&#34;
        if tr[subspace[&#34;dimensions&#34;][name]].present():
                return &#34;Dimension&#34;
        for _ in tr[subspace.range((&#34;tensors&#34;, name))]:
                return &#34;Tensor&#34;
        return None

@fdb.transactional
def read_entity(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, name: str) -&gt; Union[shoji.Dimension, shoji.WorkspaceManager, shoji.Tensor, None]:
        subspace = wsm._subspace
        if subspace.exists(tr, name):
                child = subspace.open(tr, name)
                return shoji.WorkspaceManager(wsm._db, child, wsm._path + (name,))
        elif tr[subspace[&#34;dimensions&#34;][name]].present():
                dim = shoji.Dimension(size=int.from_bytes(tr[subspace[&#34;dimensions&#34;][name]], &#34;little&#34;, signed=True))
                dim.assigned_name = name
                dim.wsm = wsm
                return dim
        else:
                # key_tuple = (&#34;tensors&#34;, name, tensor.dtype, tensor.rank) + tensor.dims + (1 if tensor.chunked else 0, len(tensor.inits))
                tensor_tuples = tr[subspace.range((&#34;tensors&#34;, name))]
                for k, _ in tensor_tuples:
                        key = subspace.unpack(k)
                        tensor = shoji.Tensor(key[2], key[4:4+key[3]], length=key[-1])
                        tensor.assigned_name = name
                        tensor.wsm = wsm
                        return tensor
        raise KeyError(f&#34;{name} not found&#34;)

@fdb.transactional
def delete_entity(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, name: str) -&gt; None:
        subspace = wsm._subspace
        if subspace.exists(tr, name):
                subspace.open(tr, name).remove(tr)
        elif tr[subspace[&#34;dimensions&#34;][name]].present():
                tr.clear_range_startswith(subspace[&#34;dimensions&#34;][name].key())
        elif tr[subspace[&#34;tensors&#34;][name]].present():
                tr.clear_range_startswith(subspace[&#34;tensors&#34;][name].key())
                tr.clear_range_startswith(subspace[&#34;tensor_values&#34;][name].key())


@fdb.transactional
def write_dimension(tr, wsm: shoji.WorkspaceManager, name: str, dim: shoji.Dimension):
        subspace = wsm._subspace
        # Check that name doesn&#39;t already exist
        existing_name = name_exists(tr, wsm, name)
        if existing_name is not None:
                raise AttributeError(f&#34;Name already exists (as {existing_name})&#34;)
        # Create the dimension
        tr[subspace[&#34;dimensions&#34;][name]] = (dim.size if dim.size is not None else -1).to_bytes(8, &#34;little&#34;, signed=True)


@fdb.transactional
def create_tensor(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, name: str, tensor: shoji.Tensor) -&gt; None:
        subspace = wsm._subspace
        # Check that name doesn&#39;t already exist
        existing_name = name_exists(tr, wsm, name)
        if existing_name is not None:
                raise AttributeError(f&#34;Name already exists (as {existing_name})&#34;)
        # Check that the dimensions of the tensor exist
        for ix, d in enumerate(tensor.dims):
                if isinstance(d, str):
                        temp = wsm[d]
                        if isinstance(temp, shoji.Dimension):
                                dim = temp
                                # Check that the dimensions of the tensor match the shape of the tensor
                                if dim.size is not None:
                                        if tensor.inits is not None and tensor.shape[ix] != dim.size:
                                                raise IndexError(f&#34;Mismatch between the declared size {dim.size} of dimension &#39;{d}&#39; and the shape {tensor.shape} of values&#34;)
                        else:
                                raise AttributeError(d)
        # Create the tensor with its initial data (if provided)
        # TODO: handle chunking
        # Store tensor definition
        length = 0
        # if tensor.inits is not None and not np.ndim(tensor.inits) == 0:
        #       length = len(tensor.inits)
        key_tuple = (&#34;tensors&#34;, name, tensor.dtype, tensor.rank) + tensor.dims + (0, length)
        key = subspace.pack(key_tuple)
        tr[key] = b&#39;&#39;


@fdb.transactional
def write_tensor_values(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, name: str, tensor: shoji.Tensor) -&gt; None:
        # TODO: write in row-ranges to avoid transaction byte limit
        CHUNK_SIZE = 1_000
        subspace = wsm._subspace
        if tensor.inits is not None:
                codec = shoji.Codec(tensor.dtype)
                if np.ndim(tensor.inits) == 0:  # It&#39;s a scalar value
                        key = subspace.pack((&#34;tensor_values&#34;, name) + (0, 0))
                        #tr[key] = blosc.pack_array(np.array(tensor.inits))
                        tr[key] = codec.encode(np.array(tensor.inits))
                else:
                        # Update the length
                        old_length: int = wsm[name].length  # type: ignore
                        length = len(tensor.inits) + old_length  # type: ignore
                        tr.clear_range_startswith(subspace[&#34;tensors&#34;][name].key())
                        key_tuple = (&#34;tensors&#34;, name, tensor.dtype, tensor.rank) + tensor.dims + (0, length)
                        key = subspace.pack(key_tuple)
                        tr[key] = b&#39;&#39;
                        for i in range(len(tensor.inits)):
                                encoded = codec.encode(np.array(tensor.inits[i]))
                                for j in range(0, len(encoded), CHUNK_SIZE):
                                        key = subspace.pack((&#34;tensor_values&#34;, name, i + old_length, j // CHUNK_SIZE))
                                        tr[key] = encoded[j:j+CHUNK_SIZE]
                        if tensor.rank == 1:
                                # Create an index
                                if tensor.dtype in (&#34;uint8&#34;, &#34;uint16&#34;, &#34;uint32&#34;, &#34;uint64&#34;, &#34;int8&#34;, &#34;int16&#34;, &#34;int32&#34;, &#34;int64&#34;):
                                        values = [int(v) for v in tensor.inits]
                                elif tensor.dtype in (&#34;float16&#34;, &#34;float32&#34;, &#34;float64&#34;):
                                        values = [float(v) for v in tensor.inits]
                                elif tensor.dtype == &#34;bool&#34;:
                                        values = [bool(v) for v in tensor.inits]
                                elif tensor.dtype == &#34;string&#34;:
                                        values = [str(v) for v in tensor.inits]
                                else:
                                        raise TypeError()
                                for i, value in enumerate(values):
                                        key = subspace.pack((&#34;tensor_indexes&#34;, name, value, int(i + old_length)))
                                        tr[key] = b&#39;&#39;


@numba.jit
def ranges(elements):
    ranges = []
    start = 0
    for ix in range(1, len(elements)):
        if elements[ix] != elements[ix - 1] + 1:
            ranges.append((start, ix - 1))
            start = ix
    ranges.append((start, len(elements)))
    return ranges


@fdb.transactional
def read_chunked_row(tr: fdb.impl.Transaction, subspace: fdb.subspace_impl.Subspace, name: str, i: int) -&gt; bytes:
        encoded = bytearray()
        j = 0
        while True:
                chunk = tr[subspace.pack((&#34;tensor_values&#34;, name, i, j))]
                if not chunk.present():
                        break
                encoded += chunk.value
                j += 1
        return bytes(encoded)


@fdb.transactional
def read_filtered_tensor(tr: fdb.impl.Transaction, subspace: fdb.subspace_impl.Subspace, name: str, tensor: shoji.Tensor, indices: np.ndarray = None) -&gt; np.ndarray:
        # TODO: read in row-ranges to avoid transaction byte limit
        # TODO: use range reads where possible to improve performance
        # TODO: use segmented reads https://apple.github.io/foundationdb/segmented-range-reads.html
        assert tensor.length is not None
        if indices is None:
                indices = np.arange(tensor.length)
        codec = shoji.Codec(tensor.dtype)
        n_rows = indices.shape[0]
        if tensor.jagged:
                resultj = []
                for i in indices:
                        resultj.append(codec.decode(read_chunked_row(tr, subspace, name, i)))
                return resultj
        else:
                result = None
                for i, j in enumerate(indices):
                        row = codec.decode(read_chunked_row(tr, subspace, name, i))
                        if result is None:
                                result = np.empty((n_rows,) + row.shape, dtype=row.dtype)
                        result[i] = row
        return result


@fdb.transactional
def const_compare(tr, wsm: shoji.WorkspaceManager, name: str, operator: str, const: Tuple[int, str, float]) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Compare a tensor to a constant value, and return all indices that match
        &#34;&#34;&#34;
        # Code for range, equality and inequality filters
        tensor = wsm[name]
        assert isinstance(tensor, shoji.Tensor)
        const = tensor.python_dtype()(const)  # Cast the const to string, float or int
        index = wsm._subspace[&#34;tensor_indexes&#34;][name]
        eq_range = index[const].range()
        all_range = index.range()
        start, stop = all_range.start, all_range.stop
        if operator == &#34;==&#34;:
                start, stop = eq_range.start, eq_range.stop
        if operator == &#34;&gt;=&#34;:
                start = eq_range.start
        elif operator == &#34;&gt;&#34;:
                start = tr.get_key(fdb.KeySelector.first_greater_than(index[const]))
        elif operator == &#34;&lt;=&#34;:
                stop = eq_range.stop
        elif operator == &#34;&lt;&#34;:
                stop = tr.get_key(fdb.KeySelector.last_less_than(index[const]))
        return np.array([index.unpack(k)[1] for k, _ in tr[start:stop]], dtype=&#34;int64&#34;)


@fdb.transactional
def append_tensors(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, dname: str, vals: Dict[str, Union[List[np.ndarray], np.ndarray]]) -&gt; None:
        &#34;&#34;&#34;
        Append values to a set of named tensors, which share their first dimension

        Args:
                tr: Transaction
                wsm: `shoji.workspace.WorkspaceManager`
                dname: Name of the dimension along which the values should be appended
                vals: dict of tensor names and corresponding values (np.ndarray or list of np.ndarray)

        Remarks:
                The function will check for the existence of all the tensors, that they have the same first dimension,
                and that no other tensor in the workspace has the same first dimension. It will also check that the values
                given have the same length along the first dimension, and that all other dimensions of each tensor match the 
                definitions of those tensors.
        &#34;&#34;&#34;
        subspace = wsm._subspace
        tensors: Dict[str, shoji.Tensor] = {}

        # Check that all the values have same length
        n_values = -1
        for values in vals.values():
                if n_values == -1:
                        n_values = len(values)
                elif len(values) != n_values:
                        raise ValueError(f&#34;Length (along first dimension) of tensors must be the same when appending&#34;)

        # Check that all named tensors exist, and have the right first dimension
        for name in vals.keys():
                if name_exists(tr, wsm, name) != &#34;Tensor&#34;:
                        raise NameError(f&#34;Tensor &#39;{name}&#39; does not exist in the workspace&#34;)
                tensors[name] = wsm[name]  # type: ignore
                if tensors[name].rank == 0 or tensors[name].dims[0] != dname:
                        raise ValueError(f&#34;Tensor &#39;{name}&#39; does not have &#39;{dname}&#39; as first dimension&#34;)
        
        # Check that all relevant tensors have been included in the values
        all_tensors: List[str] = [subspace[&#34;tensors&#34;].unpack(k)[0] for k,v in tr[subspace[&#34;tensors&#34;].range()]]
        for tensor_name in all_tensors:
                if tensor_name not in vals:
                        tensor: shoji.Tensor = wsm[tensor_name]  # type: ignore
                        if tensor.rank != 0 and tensor.dims[0] == dname:
                                raise ValueError(f&#34;Tensor &#39;{tensor.assigned_name}&#39; missing from values in append operation&#34;)

        # Check the rank of the inits
        for name, values in vals.items():
                if tensors[name].jagged:
                        for j in values:
                                if tensors[name].rank != values.ndim + 1:  # type: ignore
                                        raise ValueError(f&#34;Tensor &#39;{name}&#39; rank {tensors[name].rank} cannot be initialized with rank-{values.ndim + 1} array&#34;)  # type: ignore
                else:
                        if tensors[name].rank != values.ndim:  # type: ignore
                                raise ValueError(f&#34;Tensor &#39;{name}&#39; rank {tensors[name].rank} cannot be initialized with rank-{values.ndim} array&#34;)  # type: ignore

        # Check that all the other dimensions match their definitions
        for name, values in vals.items():
                for i, idim in enumerate(tensors[name].dims):
                        if idim is None:
                                continue
                        if isinstance(idim, str):  # Named dimension
                                dim: shoji.Dimension = wsm[idim]  # type: ignore # TODO: check that this runs in the same transaction
                                if dim.size is not None:
                                        if tensors[name].jagged:
                                                for j in values:
                                                        if dim.size != values[j].shape[i - 1]:
                                                                raise ValueError(f&#34;Tensor &#39;{name}&#39; dimension &#39;{idim}&#39; must be exactly {dim.size} elements long&#34;)
                                        else:
                                                if dim.size != values.shape[i]:  # type: ignore
                                                        raise ValueError(f&#34;Tensor &#39;{name}&#39; dimension &#39;{idim}&#39; must be exactly {dim.size} elements long&#34;)

        # Finally, write the values
        for name, values in vals.items():
                tensors[name].inits = values
                write_tensor_values(tr, wsm, name, tensors[name])


def __nil__():
        pass</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="shoji.io.append_tensors"><code class="name flex">
<span>def <span class="ident">append_tensors</span></span>(<span>tr: fdb.impl.Transaction, wsm: <a title="shoji.workspace.WorkspaceManager" href="workspace.html#shoji.workspace.WorkspaceManager">WorkspaceManager</a>, dname: str, vals: Dict[str, Union[List[numpy.ndarray], numpy.ndarray]]) -> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Append values to a set of named tensors, which share their first dimension</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tr</code></strong></dt>
<dd>Transaction</dd>
<dt><strong><code>wsm</code></strong></dt>
<dd><code><a title="shoji.workspace.WorkspaceManager" href="workspace.html#shoji.workspace.WorkspaceManager">WorkspaceManager</a></code></dd>
<dt><strong><code>dname</code></strong></dt>
<dd>Name of the dimension along which the values should be appended</dd>
<dt><strong><code>vals</code></strong></dt>
<dd>dict of tensor names and corresponding values (np.ndarray or list of np.ndarray)</dd>
</dl>
<h2 id="remarks">Remarks</h2>
<p>The function will check for the existence of all the tensors, that they have the same first dimension,
and that no other tensor in the workspace has the same first dimension. It will also check that the values
given have the same length along the first dimension, and that all other dimensions of each tensor match the
definitions of those tensors.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@fdb.transactional
def append_tensors(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, dname: str, vals: Dict[str, Union[List[np.ndarray], np.ndarray]]) -&gt; None:
        &#34;&#34;&#34;
        Append values to a set of named tensors, which share their first dimension

        Args:
                tr: Transaction
                wsm: `shoji.workspace.WorkspaceManager`
                dname: Name of the dimension along which the values should be appended
                vals: dict of tensor names and corresponding values (np.ndarray or list of np.ndarray)

        Remarks:
                The function will check for the existence of all the tensors, that they have the same first dimension,
                and that no other tensor in the workspace has the same first dimension. It will also check that the values
                given have the same length along the first dimension, and that all other dimensions of each tensor match the 
                definitions of those tensors.
        &#34;&#34;&#34;
        subspace = wsm._subspace
        tensors: Dict[str, shoji.Tensor] = {}

        # Check that all the values have same length
        n_values = -1
        for values in vals.values():
                if n_values == -1:
                        n_values = len(values)
                elif len(values) != n_values:
                        raise ValueError(f&#34;Length (along first dimension) of tensors must be the same when appending&#34;)

        # Check that all named tensors exist, and have the right first dimension
        for name in vals.keys():
                if name_exists(tr, wsm, name) != &#34;Tensor&#34;:
                        raise NameError(f&#34;Tensor &#39;{name}&#39; does not exist in the workspace&#34;)
                tensors[name] = wsm[name]  # type: ignore
                if tensors[name].rank == 0 or tensors[name].dims[0] != dname:
                        raise ValueError(f&#34;Tensor &#39;{name}&#39; does not have &#39;{dname}&#39; as first dimension&#34;)
        
        # Check that all relevant tensors have been included in the values
        all_tensors: List[str] = [subspace[&#34;tensors&#34;].unpack(k)[0] for k,v in tr[subspace[&#34;tensors&#34;].range()]]
        for tensor_name in all_tensors:
                if tensor_name not in vals:
                        tensor: shoji.Tensor = wsm[tensor_name]  # type: ignore
                        if tensor.rank != 0 and tensor.dims[0] == dname:
                                raise ValueError(f&#34;Tensor &#39;{tensor.assigned_name}&#39; missing from values in append operation&#34;)

        # Check the rank of the inits
        for name, values in vals.items():
                if tensors[name].jagged:
                        for j in values:
                                if tensors[name].rank != values.ndim + 1:  # type: ignore
                                        raise ValueError(f&#34;Tensor &#39;{name}&#39; rank {tensors[name].rank} cannot be initialized with rank-{values.ndim + 1} array&#34;)  # type: ignore
                else:
                        if tensors[name].rank != values.ndim:  # type: ignore
                                raise ValueError(f&#34;Tensor &#39;{name}&#39; rank {tensors[name].rank} cannot be initialized with rank-{values.ndim} array&#34;)  # type: ignore

        # Check that all the other dimensions match their definitions
        for name, values in vals.items():
                for i, idim in enumerate(tensors[name].dims):
                        if idim is None:
                                continue
                        if isinstance(idim, str):  # Named dimension
                                dim: shoji.Dimension = wsm[idim]  # type: ignore # TODO: check that this runs in the same transaction
                                if dim.size is not None:
                                        if tensors[name].jagged:
                                                for j in values:
                                                        if dim.size != values[j].shape[i - 1]:
                                                                raise ValueError(f&#34;Tensor &#39;{name}&#39; dimension &#39;{idim}&#39; must be exactly {dim.size} elements long&#34;)
                                        else:
                                                if dim.size != values.shape[i]:  # type: ignore
                                                        raise ValueError(f&#34;Tensor &#39;{name}&#39; dimension &#39;{idim}&#39; must be exactly {dim.size} elements long&#34;)

        # Finally, write the values
        for name, values in vals.items():
                tensors[name].inits = values
                write_tensor_values(tr, wsm, name, tensors[name])</code></pre>
</details>
</dd>
<dt id="shoji.io.const_compare"><code class="name flex">
<span>def <span class="ident">const_compare</span></span>(<span>tr, wsm: <a title="shoji.workspace.WorkspaceManager" href="workspace.html#shoji.workspace.WorkspaceManager">WorkspaceManager</a>, name: str, operator: str, const: Tuple[int, str, float]) -> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Compare a tensor to a constant value, and return all indices that match</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@fdb.transactional
def const_compare(tr, wsm: shoji.WorkspaceManager, name: str, operator: str, const: Tuple[int, str, float]) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Compare a tensor to a constant value, and return all indices that match
        &#34;&#34;&#34;
        # Code for range, equality and inequality filters
        tensor = wsm[name]
        assert isinstance(tensor, shoji.Tensor)
        const = tensor.python_dtype()(const)  # Cast the const to string, float or int
        index = wsm._subspace[&#34;tensor_indexes&#34;][name]
        eq_range = index[const].range()
        all_range = index.range()
        start, stop = all_range.start, all_range.stop
        if operator == &#34;==&#34;:
                start, stop = eq_range.start, eq_range.stop
        if operator == &#34;&gt;=&#34;:
                start = eq_range.start
        elif operator == &#34;&gt;&#34;:
                start = tr.get_key(fdb.KeySelector.first_greater_than(index[const]))
        elif operator == &#34;&lt;=&#34;:
                stop = eq_range.stop
        elif operator == &#34;&lt;&#34;:
                stop = tr.get_key(fdb.KeySelector.last_less_than(index[const]))
        return np.array([index.unpack(k)[1] for k, _ in tr[start:stop]], dtype=&#34;int64&#34;)</code></pre>
</details>
</dd>
<dt id="shoji.io.create_tensor"><code class="name flex">
<span>def <span class="ident">create_tensor</span></span>(<span>tr: fdb.impl.Transaction, wsm: <a title="shoji.workspace.WorkspaceManager" href="workspace.html#shoji.workspace.WorkspaceManager">WorkspaceManager</a>, name: str, tensor: <a title="shoji.tensor.Tensor" href="tensor.html#shoji.tensor.Tensor">Tensor</a>) -> NoneType</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@fdb.transactional
def create_tensor(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, name: str, tensor: shoji.Tensor) -&gt; None:
        subspace = wsm._subspace
        # Check that name doesn&#39;t already exist
        existing_name = name_exists(tr, wsm, name)
        if existing_name is not None:
                raise AttributeError(f&#34;Name already exists (as {existing_name})&#34;)
        # Check that the dimensions of the tensor exist
        for ix, d in enumerate(tensor.dims):
                if isinstance(d, str):
                        temp = wsm[d]
                        if isinstance(temp, shoji.Dimension):
                                dim = temp
                                # Check that the dimensions of the tensor match the shape of the tensor
                                if dim.size is not None:
                                        if tensor.inits is not None and tensor.shape[ix] != dim.size:
                                                raise IndexError(f&#34;Mismatch between the declared size {dim.size} of dimension &#39;{d}&#39; and the shape {tensor.shape} of values&#34;)
                        else:
                                raise AttributeError(d)
        # Create the tensor with its initial data (if provided)
        # TODO: handle chunking
        # Store tensor definition
        length = 0
        # if tensor.inits is not None and not np.ndim(tensor.inits) == 0:
        #       length = len(tensor.inits)
        key_tuple = (&#34;tensors&#34;, name, tensor.dtype, tensor.rank) + tensor.dims + (0, length)
        key = subspace.pack(key_tuple)
        tr[key] = b&#39;&#39;</code></pre>
</details>
</dd>
<dt id="shoji.io.delete_entity"><code class="name flex">
<span>def <span class="ident">delete_entity</span></span>(<span>tr: fdb.impl.Transaction, wsm: <a title="shoji.workspace.WorkspaceManager" href="workspace.html#shoji.workspace.WorkspaceManager">WorkspaceManager</a>, name: str) -> NoneType</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@fdb.transactional
def delete_entity(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, name: str) -&gt; None:
        subspace = wsm._subspace
        if subspace.exists(tr, name):
                subspace.open(tr, name).remove(tr)
        elif tr[subspace[&#34;dimensions&#34;][name]].present():
                tr.clear_range_startswith(subspace[&#34;dimensions&#34;][name].key())
        elif tr[subspace[&#34;tensors&#34;][name]].present():
                tr.clear_range_startswith(subspace[&#34;tensors&#34;][name].key())
                tr.clear_range_startswith(subspace[&#34;tensor_values&#34;][name].key())</code></pre>
</details>
</dd>
<dt id="shoji.io.name_exists"><code class="name flex">
<span>def <span class="ident">name_exists</span></span>(<span>tr: fdb.impl.Transaction, wsm: <a title="shoji.workspace.WorkspaceManager" href="workspace.html#shoji.workspace.WorkspaceManager">WorkspaceManager</a>, name: str) -> Union[str, NoneType]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@fdb.transactional
def name_exists(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, name: str) -&gt; Optional[str]:
        subspace = wsm._subspace
        if subspace.exists(tr, wsm._path + (name,)):
                return &#34;Workspace&#34;
        if tr[subspace[&#34;dimensions&#34;][name]].present():
                return &#34;Dimension&#34;
        for _ in tr[subspace.range((&#34;tensors&#34;, name))]:
                return &#34;Tensor&#34;
        return None</code></pre>
</details>
</dd>
<dt id="shoji.io.ranges"><code class="name flex">
<span>def <span class="ident">ranges</span></span>(<span>elements)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@numba.jit
def ranges(elements):
    ranges = []
    start = 0
    for ix in range(1, len(elements)):
        if elements[ix] != elements[ix - 1] + 1:
            ranges.append((start, ix - 1))
            start = ix
    ranges.append((start, len(elements)))
    return ranges</code></pre>
</details>
</dd>
<dt id="shoji.io.read_chunked_row"><code class="name flex">
<span>def <span class="ident">read_chunked_row</span></span>(<span>tr: fdb.impl.Transaction, subspace: fdb.subspace_impl.Subspace, name: str, i: int) -> bytes</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@fdb.transactional
def read_chunked_row(tr: fdb.impl.Transaction, subspace: fdb.subspace_impl.Subspace, name: str, i: int) -&gt; bytes:
        encoded = bytearray()
        j = 0
        while True:
                chunk = tr[subspace.pack((&#34;tensor_values&#34;, name, i, j))]
                if not chunk.present():
                        break
                encoded += chunk.value
                j += 1
        return bytes(encoded)</code></pre>
</details>
</dd>
<dt id="shoji.io.read_entity"><code class="name flex">
<span>def <span class="ident">read_entity</span></span>(<span>tr: fdb.impl.Transaction, wsm: <a title="shoji.workspace.WorkspaceManager" href="workspace.html#shoji.workspace.WorkspaceManager">WorkspaceManager</a>, name: str) -> Union[<a title="shoji.dimension.Dimension" href="dimension.html#shoji.dimension.Dimension">Dimension</a>, <a title="shoji.workspace.WorkspaceManager" href="workspace.html#shoji.workspace.WorkspaceManager">WorkspaceManager</a>, <a title="shoji.tensor.Tensor" href="tensor.html#shoji.tensor.Tensor">Tensor</a>, NoneType]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@fdb.transactional
def read_entity(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, name: str) -&gt; Union[shoji.Dimension, shoji.WorkspaceManager, shoji.Tensor, None]:
        subspace = wsm._subspace
        if subspace.exists(tr, name):
                child = subspace.open(tr, name)
                return shoji.WorkspaceManager(wsm._db, child, wsm._path + (name,))
        elif tr[subspace[&#34;dimensions&#34;][name]].present():
                dim = shoji.Dimension(size=int.from_bytes(tr[subspace[&#34;dimensions&#34;][name]], &#34;little&#34;, signed=True))
                dim.assigned_name = name
                dim.wsm = wsm
                return dim
        else:
                # key_tuple = (&#34;tensors&#34;, name, tensor.dtype, tensor.rank) + tensor.dims + (1 if tensor.chunked else 0, len(tensor.inits))
                tensor_tuples = tr[subspace.range((&#34;tensors&#34;, name))]
                for k, _ in tensor_tuples:
                        key = subspace.unpack(k)
                        tensor = shoji.Tensor(key[2], key[4:4+key[3]], length=key[-1])
                        tensor.assigned_name = name
                        tensor.wsm = wsm
                        return tensor
        raise KeyError(f&#34;{name} not found&#34;)</code></pre>
</details>
</dd>
<dt id="shoji.io.read_filtered_tensor"><code class="name flex">
<span>def <span class="ident">read_filtered_tensor</span></span>(<span>tr: fdb.impl.Transaction, subspace: fdb.subspace_impl.Subspace, name: str, tensor: <a title="shoji.tensor.Tensor" href="tensor.html#shoji.tensor.Tensor">Tensor</a>, indices: numpy.ndarray = None) -> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@fdb.transactional
def read_filtered_tensor(tr: fdb.impl.Transaction, subspace: fdb.subspace_impl.Subspace, name: str, tensor: shoji.Tensor, indices: np.ndarray = None) -&gt; np.ndarray:
        # TODO: read in row-ranges to avoid transaction byte limit
        # TODO: use range reads where possible to improve performance
        # TODO: use segmented reads https://apple.github.io/foundationdb/segmented-range-reads.html
        assert tensor.length is not None
        if indices is None:
                indices = np.arange(tensor.length)
        codec = shoji.Codec(tensor.dtype)
        n_rows = indices.shape[0]
        if tensor.jagged:
                resultj = []
                for i in indices:
                        resultj.append(codec.decode(read_chunked_row(tr, subspace, name, i)))
                return resultj
        else:
                result = None
                for i, j in enumerate(indices):
                        row = codec.decode(read_chunked_row(tr, subspace, name, i))
                        if result is None:
                                result = np.empty((n_rows,) + row.shape, dtype=row.dtype)
                        result[i] = row
        return result</code></pre>
</details>
</dd>
<dt id="shoji.io.write_dimension"><code class="name flex">
<span>def <span class="ident">write_dimension</span></span>(<span>tr, wsm: <a title="shoji.workspace.WorkspaceManager" href="workspace.html#shoji.workspace.WorkspaceManager">WorkspaceManager</a>, name: str, dim: <a title="shoji.dimension.Dimension" href="dimension.html#shoji.dimension.Dimension">Dimension</a>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@fdb.transactional
def write_dimension(tr, wsm: shoji.WorkspaceManager, name: str, dim: shoji.Dimension):
        subspace = wsm._subspace
        # Check that name doesn&#39;t already exist
        existing_name = name_exists(tr, wsm, name)
        if existing_name is not None:
                raise AttributeError(f&#34;Name already exists (as {existing_name})&#34;)
        # Create the dimension
        tr[subspace[&#34;dimensions&#34;][name]] = (dim.size if dim.size is not None else -1).to_bytes(8, &#34;little&#34;, signed=True)</code></pre>
</details>
</dd>
<dt id="shoji.io.write_tensor_values"><code class="name flex">
<span>def <span class="ident">write_tensor_values</span></span>(<span>tr: fdb.impl.Transaction, wsm: <a title="shoji.workspace.WorkspaceManager" href="workspace.html#shoji.workspace.WorkspaceManager">WorkspaceManager</a>, name: str, tensor: <a title="shoji.tensor.Tensor" href="tensor.html#shoji.tensor.Tensor">Tensor</a>) -> NoneType</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@fdb.transactional
def write_tensor_values(tr: fdb.impl.Transaction, wsm: shoji.WorkspaceManager, name: str, tensor: shoji.Tensor) -&gt; None:
        # TODO: write in row-ranges to avoid transaction byte limit
        CHUNK_SIZE = 1_000
        subspace = wsm._subspace
        if tensor.inits is not None:
                codec = shoji.Codec(tensor.dtype)
                if np.ndim(tensor.inits) == 0:  # It&#39;s a scalar value
                        key = subspace.pack((&#34;tensor_values&#34;, name) + (0, 0))
                        #tr[key] = blosc.pack_array(np.array(tensor.inits))
                        tr[key] = codec.encode(np.array(tensor.inits))
                else:
                        # Update the length
                        old_length: int = wsm[name].length  # type: ignore
                        length = len(tensor.inits) + old_length  # type: ignore
                        tr.clear_range_startswith(subspace[&#34;tensors&#34;][name].key())
                        key_tuple = (&#34;tensors&#34;, name, tensor.dtype, tensor.rank) + tensor.dims + (0, length)
                        key = subspace.pack(key_tuple)
                        tr[key] = b&#39;&#39;
                        for i in range(len(tensor.inits)):
                                encoded = codec.encode(np.array(tensor.inits[i]))
                                for j in range(0, len(encoded), CHUNK_SIZE):
                                        key = subspace.pack((&#34;tensor_values&#34;, name, i + old_length, j // CHUNK_SIZE))
                                        tr[key] = encoded[j:j+CHUNK_SIZE]
                        if tensor.rank == 1:
                                # Create an index
                                if tensor.dtype in (&#34;uint8&#34;, &#34;uint16&#34;, &#34;uint32&#34;, &#34;uint64&#34;, &#34;int8&#34;, &#34;int16&#34;, &#34;int32&#34;, &#34;int64&#34;):
                                        values = [int(v) for v in tensor.inits]
                                elif tensor.dtype in (&#34;float16&#34;, &#34;float32&#34;, &#34;float64&#34;):
                                        values = [float(v) for v in tensor.inits]
                                elif tensor.dtype == &#34;bool&#34;:
                                        values = [bool(v) for v in tensor.inits]
                                elif tensor.dtype == &#34;string&#34;:
                                        values = [str(v) for v in tensor.inits]
                                else:
                                        raise TypeError()
                                for i, value in enumerate(values):
                                        key = subspace.pack((&#34;tensor_indexes&#34;, name, value, int(i + old_length)))
                                        tr[key] = b&#39;&#39;</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="shoji" href="index.html">shoji</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="shoji.io.append_tensors" href="#shoji.io.append_tensors">append_tensors</a></code></li>
<li><code><a title="shoji.io.const_compare" href="#shoji.io.const_compare">const_compare</a></code></li>
<li><code><a title="shoji.io.create_tensor" href="#shoji.io.create_tensor">create_tensor</a></code></li>
<li><code><a title="shoji.io.delete_entity" href="#shoji.io.delete_entity">delete_entity</a></code></li>
<li><code><a title="shoji.io.name_exists" href="#shoji.io.name_exists">name_exists</a></code></li>
<li><code><a title="shoji.io.ranges" href="#shoji.io.ranges">ranges</a></code></li>
<li><code><a title="shoji.io.read_chunked_row" href="#shoji.io.read_chunked_row">read_chunked_row</a></code></li>
<li><code><a title="shoji.io.read_entity" href="#shoji.io.read_entity">read_entity</a></code></li>
<li><code><a title="shoji.io.read_filtered_tensor" href="#shoji.io.read_filtered_tensor">read_filtered_tensor</a></code></li>
<li><code><a title="shoji.io.write_dimension" href="#shoji.io.write_dimension">write_dimension</a></code></li>
<li><code><a title="shoji.io.write_tensor_values" href="#shoji.io.write_tensor_values">write_tensor_values</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>