<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.4" />
<title>shoji.tensor API documentation</title>
<meta name="description" content="All data in Shoji is stored as N-dimensional tensors. A tensor is a
generalisation of scalars, vectors and matrices to N dimensions â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>shoji.tensor</code></h1>
</header>
<section id="section-intro">
<p>All data in Shoji is stored as N-dimensional tensors. A tensor is a
generalisation of scalars, vectors and matrices to N dimensions. </p>
<p>Tensors are defined by their <em>rank</em>, <em>datatype</em>, <em>dimensions</em> and <em>shape</em>.
In addition, tensors can be <em>jagged</em> (i.e. some dimensions have non-uniform
sizes).</p>
<p>Tensors can be extended along any of their dimensions (unless the
dimension is declared fixed-length) by appending values. </p>
<h2 id="overview">Overview</h2>
<p>Tensors are created like this:</p>
<pre><code class="python">import shoji
tissues = ...        # Assume we have an np.ndarray of tissue names
db = shoji.connect() # Connect to the database
ws = db.scRNA        # scRNA is a Workspace in the database, previously created

ws.Tissue = shoji.Tensor(&quot;string&quot;, (&quot;cells&quot;,), inits=tissues)
</code></pre>
<p>The tensor is declared with a datatype <code>"string"</code>, a tuple of dimensions <code>("cells",)</code> and an optional <code>np.ndarray</code> of initial values.</p>
<h3 id="rank">Rank</h3>
<p>The <em>rank</em> of a tensor is the number of dimensions of the tensor. A scalar
value has rank 0, a vector has rank 1, and a matrix has rank 2. Higher ranks
are possible; for example, a vector of 2D images would have rank 3, and a
timelapse recording in several color channels would have rank 4 (timepoint, x, y,
color).</p>
<h3 id="datatype">Datatype</h3>
<p>Tensors support the following datatypes: </p>
<pre><code class="python">&quot;bool&quot;
&quot;uint8&quot;, &quot;uint16&quot;, &quot;uint32&quot;, &quot;uint64&quot;
&quot;int8&quot;, &quot;int16&quot;, &quot;int32&quot;, &quot;int64&quot;
&quot;float16&quot;, &quot;float32&quot;, &quot;float64&quot;
&quot;string&quot;
</code></pre>
<p>The datatype of a tensor must always be declared; there is no default type.</p>
<p>When a tensor is created, any initial values provided (via the <code>inits</code> argument)
must have the matching numpy datatype. The bool and numeric datatypes match 1:1 with numpy dtypes. </p>
<p>However, the Shoji <code>string</code> datatype is a Unicode string of variable length,
which corresponds to a numpy array of string objects. That is, the corresponding
numpy datatype is <em>not</em> <code>str</code> or <code>"unicode"</code>. Instead, Shoji string tensors correspond
to numpy <code>object</code> arrays whose elements are Python <code>str</code> objects. You can cast a numpy
<code>str</code> array to an <code>object</code> array as follows:</p>
<pre><code class="python">import numpy as np
s = np.array([&quot;dog&quot;, &quot;cat&quot;, &quot;apple&quot;, &quot;orange&quot;])  # s.dtype.kind == 'U'
t = s.astype(object)  # t.dtype.kind == 'O'
# Or directly, using dtype
s = np.array([&quot;dog&quot;, &quot;cat&quot;, &quot;apple&quot;, &quot;orange&quot;], dtype=&quot;object&quot;)
</code></pre>
<p>The reason for this discrepancy is that numpy <code>str</code> arrays store only
fixed-length strings, whereas Shoji <code>string</code> tensors store strings of variable length.</p>
<h3 id="dimensions">Dimensions</h3>
<p>When creating a tensor, its dimensions must be declared using a tuple.
Scalars have rank zero, and are declared with the empty tuple <code>()</code>.
Vectors have rank one, and are declared with a single-element tuple, e.g.
<code>(20,)</code> (note the comma, which is necessary). Matrices have rank 2, and are
declared with a two-element tuple, e.g. <code>(20, 40)</code>. Higher-rank tensors are
declared with correspondingly longer tuples. </p>
<p>Dimensions can be fixed or variable-length. A fixed-length dimension is
declared with an integer specifying the number of elements of the dimension.
A variable-length dimension is declared as <code>None</code>. For example, <code>(10, None)</code>
is a matrix with ten rows and a variable number of columns. </p>
<p>The meaning of a variable-length dimension is slightly different for regular
and jagged tensors. For a regular tensor, if a dimension is variable-length then
the tensor can be extended along that dimension by appending data. Thus a tensor declared
with <code>dims=(None, 10)</code> at any point in time has a fixed number of
rows and columns, but rows can be appended (see <code><a title="shoji.dimension" href="dimension.html">shoji.dimension</a></code> and
<code><a title="shoji.dimension.Dimension.append" href="dimension.html#shoji.dimension.Dimension.append">Dimension.append()</a></code>).</p>
<p>If the tensor is jagged, then a variable-length dimension can contain
individual rows (columns, etc) of different lengths.</p>
<p>Each dimension of a tensor can be named, and named dimensions (within a
<code><a title="shoji.workspace.Workspace" href="workspace.html#shoji.workspace.Workspace">Workspace</a></code>) are constrained to have the same number of elements.
For example, if two tensors are declared with dimensions <code>("cells",)</code>
and <code>("cells", "genes")</code>, then the first dimensions are guaranteed to have
the same number of elements, which are assumed to be in the same order.</p>
<p>Named dimensions must be declared before they are used; see <code><a title="shoji.dimension" href="dimension.html">shoji.dimension</a></code>.</p>
<h3 id="shape">Shape</h3>
<p>The <code>shape</code> of a <code><a title="shoji.tensor.Tensor" href="#shoji.tensor.Tensor">Tensor</a></code> is a tuple of integers that gives the current
shape of the tensor as stored in the database. For example, a tensor with <code>dims=(None, 10, 20)</code>
might have <code>shape=(10, 10, 20)</code>, indicating that currently the tensor has ten rows. Since
the first dimension is variable-length (in this case), rows might be appended later, and the shape
would change to reflect the new number of rows.</p>
<h2 id="chunks">Chunks</h2>
<p>Data in Shoji is stored and retrieved as N-dimensional chunks. When you
read or write from a tensor, your operations are converted to operations
on chunks. For example, if you access a single element of a matrix, under
the hood the whole chunk containing the element is retrieved. </p>
<p>When you create a tensor, you can optionally specify the chunk size along
each dimension. Chunking is <strong>very</strong> important for performance. Small chunks
such as (10,100) or even (1, 100) can be an order of magnitude
faster for random access, but an order of magnitude slower for contiguous
access, as compared to large chunks like (100, 1000) or (1000, 1000). If
you know that you will only read in large contiguous blocks, use large chunks
along those dimensions. If you know you will be reading many randomly
placed single or few indices, use small chunks along those dimensions.</p>
<h2 id="reading-from-tensors">Reading from tensors</h2>
<p>The universal method for reading data in shoji is to create a <code><a title="shoji.view.View" href="view.html#shoji.view.View">View</a></code>
of the workspace. However, sometimes you just want to read from one tensor
and don't care about creating a view. Shoji supports indexing tensors similar
to numpy "fancy indexing" (and similar to how views are created):</p>
<pre><code class="python">x = ws.Expression[:]  # Read the whole tensor
y = ws.Expression[10:20]  # Read a slice
z = ws.Expression[(1, 2, 5, 9)]  # Read specific rows
w = ws.Expression[(True, False, True)]  # Read rows given by bool mask array
</code></pre>
<p>The above expressions are just shorthands for creating the corresponding view
and immediately reading from the tensor. There is no difference in performance.
For example, these two expressions below are equivalent:</p>
<pre><code class="python">x = ws.Expression[:]
x = ws[:].Expression
</code></pre>
<h2 id="jagged-tensors">Jagged tensors</h2>
<p>If a tensor is declared <em>jagged</em>, the size along variable-length dimensions
can be different for different rows (columns, etc.). For example:</p>
<pre><code class="python">ws.cells = shoji.Dimension(shape=None)
ws.Image = shoji.Tensor(&quot;uint16&quot;, (&quot;cells&quot;, None, None), jagged=True)
</code></pre>
<p>In this example, we declare a 3D jagged tensor <code>Image</code>, where dimensions 2 and 3
are variable-length. This could be used to store 2D images of cells, each of which
has a different width and height. The first dimension represents the objects
(individual cells) and the 2nd and 3rd dimensions represent the images. Accessing
a single row of this tensor would return a single 2D image matrix. Accessing a set
of rows would return a list of 2D images.</p>
<p>In a similar way, we could store multichannel timelapse images of cells:</p>
<pre><code class="python">ws.cells = shoji.Dimension(shape=None)
ws.channels = shoji.Dimension(shape=3)
ws.timepoints = shoji.Dimension(shape=1200)  # 1200 timepoints
ws.Image = shoji.Tensor(&quot;uint16&quot;, (&quot;cells&quot;, &quot;channels&quot;, &quot;timepoints&quot;, None, None), jagged=True)
</code></pre>
<p>In this examples, <code>Image</code> is a 5-dimensional tensor, where the last two dimensions
have variable length.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
All data in Shoji is stored as N-dimensional tensors. A tensor is a
generalisation of scalars, vectors and matrices to N dimensions. 

Tensors are defined by their *rank*, *datatype*, *dimensions* and *shape*.
In addition, tensors can be *jagged* (i.e. some dimensions have non-uniform
sizes).

Tensors can be extended along any of their dimensions (unless the 
dimension is declared fixed-length) by appending values. 

## Overview

Tensors are created like this:

```python
import shoji
tissues = ...        # Assume we have an np.ndarray of tissue names
db = shoji.connect() # Connect to the database
ws = db.scRNA        # scRNA is a Workspace in the database, previously created

ws.Tissue = shoji.Tensor(&#34;string&#34;, (&#34;cells&#34;,), inits=tissues)
```

The tensor is declared with a datatype `&#34;string&#34;`, a tuple of dimensions `(&#34;cells&#34;,)` and an optional `np.ndarray` of initial values.

### Rank

The *rank* of a tensor is the number of dimensions of the tensor. A scalar 
value has rank 0, a vector has rank 1, and a matrix has rank 2. Higher ranks
are possible; for example, a vector of 2D images would have rank 3, and a
timelapse recording in several color channels would have rank 4 (timepoint, x, y,
color).

### Datatype

Tensors support the following datatypes: 

```python
&#34;bool&#34;
&#34;uint8&#34;, &#34;uint16&#34;, &#34;uint32&#34;, &#34;uint64&#34;
&#34;int8&#34;, &#34;int16&#34;, &#34;int32&#34;, &#34;int64&#34;
&#34;float16&#34;, &#34;float32&#34;, &#34;float64&#34;
&#34;string&#34;
```

The datatype of a tensor must always be declared; there is no default type.

When a tensor is created, any initial values provided (via the `inits` argument)
must have the matching numpy datatype. The bool and numeric datatypes match 1:1 with numpy dtypes. 

However, the Shoji `string` datatype is a Unicode string of variable length,
which corresponds to a numpy array of string objects. That is, the corresponding
numpy datatype is *not* `str` or `&#34;unicode&#34;`. Instead, Shoji string tensors correspond
to numpy `object` arrays whose elements are Python `str` objects. You can cast a numpy
`str` array to an `object` array as follows:

```python
import numpy as np
s = np.array([&#34;dog&#34;, &#34;cat&#34;, &#34;apple&#34;, &#34;orange&#34;])  # s.dtype.kind == &#39;U&#39;
t = s.astype(object)  # t.dtype.kind == &#39;O&#39;
# Or directly, using dtype
s = np.array([&#34;dog&#34;, &#34;cat&#34;, &#34;apple&#34;, &#34;orange&#34;], dtype=&#34;object&#34;)
```

The reason for this discrepancy is that numpy `str` arrays store only
fixed-length strings, whereas Shoji `string` tensors store strings of variable length.

### Dimensions

When creating a tensor, its dimensions must be declared using a tuple. 
Scalars have rank zero, and are declared with the empty tuple `()`. 
Vectors have rank one, and are declared with a single-element tuple, e.g. 
`(20,)` (note the comma, which is necessary). Matrices have rank 2, and are
declared with a two-element tuple, e.g. `(20, 40)`. Higher-rank tensors are 
declared with correspondingly longer tuples. 

Dimensions can be fixed or variable-length. A fixed-length dimension is 
declared with an integer specifying the number of elements of the dimension.
A variable-length dimension is declared as `None`. For example, `(10, None)` 
is a matrix with ten rows and a variable number of columns. 

The meaning of a variable-length dimension is slightly different for regular
and jagged tensors. For a regular tensor, if a dimension is variable-length then 
the tensor can be extended along that dimension by appending data. Thus a tensor declared 
with `dims=(None, 10)` at any point in time has a fixed number of
rows and columns, but rows can be appended (see `shoji.dimension` and 
`shoji.dimension.Dimension.append`).

If the tensor is jagged, then a variable-length dimension can contain 
individual rows (columns, etc) of different lengths.

Each dimension of a tensor can be named, and named dimensions (within a 
`shoji.workspace.Workspace`) are constrained to have the same number of elements. 
For example, if two tensors are declared with dimensions `(&#34;cells&#34;,)` 
and `(&#34;cells&#34;, &#34;genes&#34;)`, then the first dimensions are guaranteed to have 
the same number of elements, which are assumed to be in the same order.

Named dimensions must be declared before they are used; see `shoji.dimension`.


### Shape

The `shape` of a `shoji.tensor.Tensor` is a tuple of integers that gives the current
shape of the tensor as stored in the database. For example, a tensor with `dims=(None, 10, 20)`
might have `shape=(10, 10, 20)`, indicating that currently the tensor has ten rows. Since
the first dimension is variable-length (in this case), rows might be appended later, and the shape
would change to reflect the new number of rows.


## Chunks

Data in Shoji is stored and retrieved as N-dimensional chunks. When you
read or write from a tensor, your operations are converted to operations
on chunks. For example, if you access a single element of a matrix, under 
the hood the whole chunk containing the element is retrieved. 

When you create a tensor, you can optionally specify the chunk size along
each dimension. Chunking is **very** important for performance. Small chunks 
such as (10,100) or even (1, 100) can be an order of magnitude 
faster for random access, but an order of magnitude slower for contiguous
access, as compared to large chunks like (100, 1000) or (1000, 1000). If
you know that you will only read in large contiguous blocks, use large chunks 
along those dimensions. If you know you will be reading many randomly 
placed single or few indices, use small chunks along those dimensions.


## Reading from tensors

The universal method for reading data in shoji is to create a `shoji.view.View`
of the workspace. However, sometimes you just want to read from one tensor
and don&#39;t care about creating a view. Shoji supports indexing tensors similar 
to numpy &#34;fancy indexing&#34; (and similar to how views are created):

```python
x = ws.Expression[:]  # Read the whole tensor
y = ws.Expression[10:20]  # Read a slice
z = ws.Expression[(1, 2, 5, 9)]  # Read specific rows
w = ws.Expression[(True, False, True)]  # Read rows given by bool mask array
```

The above expressions are just shorthands for creating the corresponding view
and immediately reading from the tensor. There is no difference in performance.
For example, these two expressions below are equivalent:

```python
x = ws.Expression[:]
x = ws[:].Expression
```

## Jagged tensors

If a tensor is declared *jagged*, the size along variable-length dimensions 
can be different for different rows (columns, etc.). For example:

```python
ws.cells = shoji.Dimension(shape=None)
ws.Image = shoji.Tensor(&#34;uint16&#34;, (&#34;cells&#34;, None, None), jagged=True)
```

In this example, we declare a 3D jagged tensor `Image`, where dimensions 2 and 3 
are variable-length. This could be used to store 2D images of cells, each of which 
has a different width and height. The first dimension represents the objects 
(individual cells) and the 2nd and 3rd dimensions represent the images. Accessing 
a single row of this tensor would return a single 2D image matrix. Accessing a set 
of rows would return a list of 2D images.

In a similar way, we could store multichannel timelapse images of cells:

```python
ws.cells = shoji.Dimension(shape=None)
ws.channels = shoji.Dimension(shape=3)
ws.timepoints = shoji.Dimension(shape=1200)  # 1200 timepoints
ws.Image = shoji.Tensor(&#34;uint16&#34;, (&#34;cells&#34;, &#34;channels&#34;, &#34;timepoints&#34;, None, None), jagged=True)
```

In this examples, `Image` is a 5-dimensional tensor, where the last two dimensions 
have variable length.
&#34;&#34;&#34;
from typing import Tuple, Union, List, Optional, Callable
try:
    from typing import Literal
except ImportError:
    from typing_extensions import Literal
import numpy as np
import shoji
import sys
import logging


FancyIndexElement = Union[&#34;shoji.Filter&#34;, slice, int, np.ndarray]
FancyIndex = Union[FancyIndexElement, Tuple[FancyIndexElement, ...]]

class TensorValue:
        def __init__(self, values: Union[Tuple[np.ndarray], List[np.ndarray], np.ndarray]) -&gt; None:
                self.values = values
                if isinstance(values, (list, tuple)):
                        self.jagged = True
                        self.rank = values[0].ndim + 1
                        self.dtype = values[0].dtype.name
                        if self.dtype == &#34;object&#34;:
                                self.dtype = &#34;string&#34;

                        shape = np.array(values[0].shape)
                        for i, array in enumerate(values):
                                if not isinstance(array, np.ndarray):
                                        raise ValueError(&#34;Rows of jagged tensor must be numpy ndarrays&#34;)
                                if self.rank != array.ndim + 1:
                                        raise ValueError(&#34;Rows of jagged tensor cannot be mixed rank&#34;)
                                if self.dtype != array.dtype:
                                        raise ValueError(&#34;Rows of jagged tensor cannot be mixed dtype&#34;)
                                if self.dtype == &#34;string&#34;:
                                        if not all([isinstance(x, str) for x in array.flat]):
                                                raise TypeError(&#34;string tensors (numpy dtype=&#39;object&#39;) must contain only string elements&#34;)
                                if array.ndim != len(shape):
                                        raise ValueError(f&#34;Rank mismatch: shape {array.shape} of subarray at row {i} is not the same rank as shape {shape} at row 0&#34;)
                                shape = np.maximum(shape, array.shape)
                        self.shape = tuple([len(values)] + list(shape))
                else:
                        self.jagged = False
                        self.rank = values.ndim
                        self.dtype = values.dtype.name
                        if self.dtype == &#34;object&#34;:
                                self.dtype = &#34;string&#34;
                                if not all([isinstance(x, str) for x in values.flat]):
                                        raise TypeError(&#34;string tensors (numpy dtype=&#39;object&#39;) must contain only string elements&#34;)
                        self.shape = values.shape

                if self.dtype not in Tensor.valid_types:
                        raise TypeError(f&#34;Invalid dtype &#39;{self.dtype}&#39; for tensor value&#34;)

        @property
        def size(self) -&gt; int:
                return np.prod(self.shape)

        def __len__(self) -&gt; int:
                if self.rank &gt; 0:
                        return self.shape[0]
                return 1
        
        def __iter__(self):
                for row in self.values:
                        yield row

        def __getitem__(self, slice_) -&gt; &#34;TensorValue&#34;:
                if self.jagged:
                        if isinstance(slice_, slice):
                                slice_ = (slice_)
                        slice_ = slice_ + (slice(None),) * (self.rank - len(slice_))
                        sliced = [vals[slice_[1:]] for vals in self.values[slice_[0]]]
                        return TensorValue(sliced)
                return TensorValue(self.values[slice_])

        def size_in_bytes(self) -&gt; int:
                n_bytes = 0
                if not self.jagged:
                        if self.dtype == &#34;string&#34;:
                                n_bytes += sum([len(s) + 1 for s in self.values]) * 2
                        else:
                                n_bytes += self.values.size * self.values.itemsize  # type: ignore
                else:
                        for row in self.values:
                                if self.dtype == &#34;string&#34;:
                                        n_bytes += sum([len(s) + 1 for s in row]) * 2
                                else:
                                        n_bytes += row.size * row.itemsize
                return n_bytes

class Tensor:
        valid_types = (&#34;bool&#34;, &#34;uint8&#34;, &#34;uint16&#34;, &#34;uint32&#34;, &#34;uint64&#34;, &#34;int8&#34;, &#34;int16&#34;, &#34;int32&#34;, &#34;int64&#34;, &#34;float16&#34;, &#34;float32&#34;, &#34;float64&#34;, &#34;string&#34;)

        def __init__(self, dtype: str, dims: Union[Tuple[Union[None, int, str], ...]], *, chunks: Tuple[int, ...] = None, jagged: bool = False, inits: Union[List[np.ndarray], np.ndarray] = None) -&gt; None:
                &#34;&#34;&#34;
                Args:
                        dtype:  string giving the datatype of the tensor elements
                        dims:   A tuple of None, int, string (empty tuple designates a scalar)
                        chunks: Tuple defining the chunk size along each dimension, or &#34;auto&#34; to use automatic chunking
                        jagged: If true, this is a jagged tensor (and inits must be a list of ndarrays)
                        inits:  Optional values to initialize the tensor with

                Remarks:
                        Dimensions are specified as:

                                None:           resizable/jagged anonymous dimension
                                int:            fixed-shape anonymous dimension
                                string:         named dimension

                        Chunking is VERY important for performance. Small chunks such as (10,100) or even (1, 100) can be an order of magnitude 
                        faster for random access, but an order of magnitude slower for contiguous access, as compared to large chunks 
                        like (100, 1000) or (1000, 1000). If you know that you will only read in large contiguous blocks, use large chunks 
                        along those dimensions. If you know you will be reading many randomly placed single or few indices, use small chunks 
                        along those dimensions.

                        For rank-0 tensors, use chunks=(1,)
                &#34;&#34;&#34;
                self.dtype = dtype
                
                # Check that the type is valid
                if dtype not in Tensor.valid_types:
                        raise TypeError(f&#34;Invalid Tensor type {dtype}&#34;)

                self.dims = dims
                self.jagged = jagged

                self.name = &#34;&#34;  # Will be set if the Tensor is read from the db
                self.wsm: Optional[shoji.WorkspaceManager] = None  # Will be set if the Tensor is read from the db

                if inits is None:
                        self.inits: Optional[TensorValue] = None
                        self.shape = (0,) * len(dims)
                else:
                        # If scalar, convert to an ndarray scalar which will have shape ()
                        if np.isscalar(inits):
                                self.inits = TensorValue(np.array(inits, dtype=self.numpy_dtype()))
                        else:
                                self.inits = TensorValue(inits)
                        if self.inits.jagged and not self.jagged:
                                raise ValueError(f&#34;Jagged inits cannot be used to create non-jagged tensor&#34;)
                        self.shape = self.inits.shape

                        if len(self.dims) != len(self.shape):
                                raise ValueError(f&#34;Rank mismatch: shape {self.dims} declared is not the same rank as shape {self.shape} of values&#34;)

                        if self.dtype != self.inits.dtype:
                                raise TypeError(f&#34;Tensor dtype &#39;{self.dtype}&#39; does not match dtype of inits &#39;{self.inits.dtype}&#39;&#34;)

                for ix, dim in enumerate(self.dims):
                        if dim is not None and not isinstance(dim, int) and not isinstance(dim, str):
                                raise ValueError(f&#34;Dimension {ix} &#39;{dim}&#39; is invalid (must be None, int or str)&#34;)

                        if isinstance(dim, int) and self.inits is not None:
                                if self.shape[ix] != dim:  # type: ignore
                                        raise IndexError(f&#34;Mismatch between the declared shape {dim}Â of dimension {ix} and the inferred shape {self.shape} of values&#34;)

                self.chunks: Tuple[int, ...] = ()
                if chunks is None:
                        if dtype in (&#34;bool&#34;, &#34;uint8&#34;, &#34;int8&#34;):
                                byte_size = 1
                        if dtype in (&#34;uint16&#34;, &#34;int16&#34;, &#34;float16&#34;):
                                byte_size = 2
                        elif dtype in (&#34;uint32&#34;, &#34;int32&#34;, &#34;float32&#34;):
                                byte_size = 4
                        elif dtype in (&#34;uint64&#34;, &#34;int64&#34;, &#34;float64&#34;):
                                byte_size = 8
                        elif dtype == &#34;string&#34;:
                                byte_size = 32  # This will fail for very long strings
                        if self.rank == 0:
                                self.chunks = ()
                        elif self.rank == 1:
                                self.chunks = (500 // byte_size,)
                        else:
                                desired_sizes = (300 // byte_size, 100) + (1,) * (self.rank - 2)
                                max_sizes = (dim if isinstance(dim, int) else sys.maxsize for dim in self.dims)
                                self.chunks = tuple(min(a,b) for a,b in zip(max_sizes, desired_sizes))
                else:
                        if len(chunks) != self.rank:
                                raise ValueError(f&#34;chunks={chunks} is wrong number of dimensions for rank-{self.rank} tensor&#34; + (&#34; (use () for rank-0 tensor)&#34; if self.rank == 0 else &#34;&#34;))
                        self.chunks = chunks
                self.initializing = False

        # Support pickling
        def __getstate__(self):
                &#34;&#34;&#34;Return state values to be pickled.&#34;&#34;&#34;
                return (self.dtype, self.jagged, self.dims, self.shape, self.chunks, self.initializing, 0)  # The extra zero is for future use as a version flag

        def __setstate__(self, state):
                &#34;&#34;&#34;Restore state from the unpickled state values.&#34;&#34;&#34;
                self.dtype, self.jagged, self.dims, self.shape, self.chunks, self.initializing, _ = state

        def __len__(self) -&gt; int:
                if self.rank &gt; 0:
                        return self.shape[0]
                return 0

        @property
        def rank(self) -&gt; int:
                return len(self.dims)

        @property
        def bytewidth(self) -&gt; int:
                if self.dtype in (&#34;bool&#34;, &#34;uint8&#34;, &#34;int8&#34;):
                        return 1
                elif self.dtype in (&#34;uint16&#34;, &#34;int16&#34;, &#34;float16&#34;):
                        return 2
                elif self.dtype in (&#34;uint32&#34;, &#34;int32&#34;, &#34;float32&#34;):
                        return 3
                elif self.dtype in (&#34;uint64&#34;, &#34;int64&#34;, &#34;float64&#34;):
                        return 4
                return -1

        def _fancy_indexing(self, expr: FancyIndex) -&gt; Tuple[&#34;shoji.Filter&#34;, ...]:
                if isinstance(expr, tuple):
                        fancyindex: Tuple[FancyIndexElement, ...] = expr
                else:
                        fancyindex = (expr,)
        
                # Fill in missing axes with : just like numpy does
                if any(isinstance(x, type(...)) for x in fancyindex):  # We can&#39;t use a simple &#34;if ... in fancyindex&#34; because fancyindex may contain numpy arrays which complain about ==
                        ix = fancyindex.index(...)
                        fancyindex = fancyindex[:ix] + (slice(None),) * (self.rank - len(fancyindex) - 1) + fancyindex[ix + 1:]

                if len(fancyindex) &lt; self.rank:
                        fancyindex += (slice(None),) * (self.rank - len(fancyindex))

                filters: List[shoji.Filter] = []
                for axis, (dim, fi) in enumerate(zip(self.dims, fancyindex)):
                        # Maybe it&#39;s a Filter?
                        if isinstance(fi, shoji.Filter):
                                if isinstance(dim, str) and isinstance(fi.dim, str) and fi.dim != dim:
                                        raise IndexError(f&#34;Tensor dimension &#39;{dim}&#39; cannot be indexed uing filter expression &#39;{fi}&#39; with dimension &#39;{fi.dim}&#39;&#34;)
                                filters.append(fi)
                        elif isinstance(fi, slice):
                                filters.append(shoji.TensorSliceFilter(self, fi, axis))
                        elif isinstance(fi, (int, np.int64, np.int32)):
                                filters.append(shoji.TensorIndicesFilter(self, np.array(fi), axis))
                        elif isinstance(fi, np.ndarray):
                                if np.issubdtype(fi.dtype, np.bool_):
                                        filters.append(shoji.TensorBoolFilter(self, fi, axis))
                                elif np.issubdtype(fi.dtype, np.int_):
                                        filters.append(shoji.TensorIndicesFilter(self, fi, axis))
                        else:
                                raise KeyError()
                return tuple(filters)

        def __getitem__(self, expr: FancyIndex) -&gt; np.ndarray:
                assert self.wsm is not None, &#34;Tensor is not bound to a database&#34;
                return shoji.View(self.wsm, self._fancy_indexing(expr))[self.name]

        def __setitem__(self, expr: FancyIndex, vals: np.ndarray) -&gt; None:
                assert self.wsm is not None, &#34;Tensor is not bound to a database&#34;
                shoji.View(self.wsm, self._fancy_indexing(expr))[self.name] = vals

        def numpy_dtype(self) -&gt; str:
                if self.dtype == &#34;string&#34;:
                        return &#34;object&#34;
                return self.dtype

        def python_dtype(self) -&gt; Callable:
                if self.dtype == &#34;string&#34;:
                        return str
                if self.dtype == &#34;bool&#34;:
                        return bool
                if self.dtype in (&#34;float16&#34;, &#34;float32&#34;, &#34;float64&#34;):
                        return float
                return int

        def _compare(self, operator, other) -&gt; &#34;shoji.Filter&#34;:
                if isinstance(other, Tensor):
                        return shoji.TensorFilter(operator, self, other)
                elif isinstance(other, (str, int, float, bool)):
                        return shoji.ConstFilter(operator, self, other)
                elif isinstance(other, np.integer):
                        return shoji.ConstFilter(operator, self, int(other))
                elif isinstance(other, np.float):
                        return shoji.ConstFilter(operator, self, float(other))
                elif isinstance(other, np.object):
                        return shoji.ConstFilter(operator, self, str(other))
                elif isinstance(other, np.bool):
                        return shoji.ConstFilter(operator, self, bool(other))
                else:
                        raise TypeError(&#34;Invalid operands for expression&#34;)

        def __eq__(self, other) -&gt; &#34;shoji.Filter&#34;:  # type: ignore
                return self._compare(&#34;==&#34;, other)
                
        def __ne__(self, other) -&gt; &#34;shoji.Filter&#34;:  # type: ignore
                return self._compare(&#34;!=&#34;, other)

        def __gt__(self, other) -&gt; &#34;shoji.Filter&#34;:  # type: ignore
                return self._compare(&#34;&gt;&#34;, other)

        def __lt__(self, other) -&gt; &#34;shoji.Filter&#34;:  # type: ignore
                return self._compare(&#34;&lt;&#34;, other)

        def __ge__(self, other) -&gt; &#34;shoji.Filter&#34;:  # type: ignore
                return self._compare(&#34;&gt;=&#34;, other)

        def __le__(self, other) -&gt; &#34;shoji.Filter&#34;:  # type: ignore
                return self._compare(&#34;&lt;=&#34;, other)

        def append(self, vals: Union[List[np.ndarray], np.ndarray], axis: int = 0) -&gt; None:
                assert self.wsm is not None, &#34;Cannot append to unbound tensor&#34;
                
                if self.rank == 0:
                        raise ValueError(&#34;Cannot append to a scalar&#34;)

                tv = TensorValue(vals)
                shoji.io.append_values_multibatch(self.wsm, [self.name], [tv], (axis,))

        def _quick_look(self) -&gt; str:
                if self.rank == 0:
                        if self.dtype == &#34;string&#34;:
                                s = f&#39;&#34;{self[:]}&#34;&#39;
                        else:
                                s = str(self[:])
                        if len(s) &gt; 60:
                                return s[:56] + &#34; ...&#34;
                        return s

                def look(vals) -&gt; str:
                        s = &#34;[&#34;
                        if not isinstance(vals, list) and vals.ndim == 1:
                                if self.dtype == &#34;string&#34;:
                                        s += &#34;, &#34;.join([f&#39;&#34;{x}&#34;&#39; for x in vals[:5]])
                                else:
                                        s += &#34;, &#34;.join([str(x) for x in vals[:5]])
                        else:
                                elms = []
                                for val in vals[:5]:
                                        elms.append(look(val))
                                s += &#34;, &#34;.join(elms)
                        if len(vals) &gt; 5:
                                s += &#34;, ...]&#34;
                        else:
                                s += &#34;]&#34;
                        return s

                s = look(self[:10])
                if len(s) &gt; 60:
                        return s[:56] + &#34; Â·Â·Â·&#34;
                return s



        def __repr__(self) -&gt; str:
                return f&#34;&lt;Tensor {self.name} dtype=&#39;{self.dtype}&#39; dims={self.dims}, shape={self.shape}, chunks={self.chunks}&gt;&#34;</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="shoji.tensor.Tensor"><code class="flex name class">
<span>class <span class="ident">Tensor</span></span>
<span>(</span><span>dtype:Â str, dims:Â Tuple[Union[NoneType,Â int,Â str],Â ...], *, chunks:Â Tuple[int,Â ...]Â =Â None, jagged:Â boolÂ =Â False, inits:Â Union[List[numpy.ndarray],Â numpy.ndarray]Â =Â None)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>dtype</code></strong></dt>
<dd>string giving the datatype of the tensor elements</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>A tuple of None, int, string (empty tuple designates a scalar)</dd>
<dt><strong><code>chunks</code></strong></dt>
<dd>Tuple defining the chunk size along each dimension, or "auto" to use automatic chunking</dd>
<dt><strong><code>jagged</code></strong></dt>
<dd>If true, this is a jagged tensor (and inits must be a list of ndarrays)</dd>
<dt><strong><code>inits</code></strong></dt>
<dd>Optional values to initialize the tensor with</dd>
</dl>
<h2 id="remarks">Remarks</h2>
<p>Dimensions are specified as:</p>
<pre><code>    None:           resizable/jagged anonymous dimension
    int:            fixed-shape anonymous dimension
    string:         named dimension
</code></pre>
<p>Chunking is VERY important for performance. Small chunks such as (10,100) or even (1, 100) can be an order of magnitude
faster for random access, but an order of magnitude slower for contiguous access, as compared to large chunks
like (100, 1000) or (1000, 1000). If you know that you will only read in large contiguous blocks, use large chunks
along those dimensions. If you know you will be reading many randomly placed single or few indices, use small chunks
along those dimensions.</p>
<p>For rank-0 tensors, use chunks=(1,)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Tensor:
        valid_types = (&#34;bool&#34;, &#34;uint8&#34;, &#34;uint16&#34;, &#34;uint32&#34;, &#34;uint64&#34;, &#34;int8&#34;, &#34;int16&#34;, &#34;int32&#34;, &#34;int64&#34;, &#34;float16&#34;, &#34;float32&#34;, &#34;float64&#34;, &#34;string&#34;)

        def __init__(self, dtype: str, dims: Union[Tuple[Union[None, int, str], ...]], *, chunks: Tuple[int, ...] = None, jagged: bool = False, inits: Union[List[np.ndarray], np.ndarray] = None) -&gt; None:
                &#34;&#34;&#34;
                Args:
                        dtype:  string giving the datatype of the tensor elements
                        dims:   A tuple of None, int, string (empty tuple designates a scalar)
                        chunks: Tuple defining the chunk size along each dimension, or &#34;auto&#34; to use automatic chunking
                        jagged: If true, this is a jagged tensor (and inits must be a list of ndarrays)
                        inits:  Optional values to initialize the tensor with

                Remarks:
                        Dimensions are specified as:

                                None:           resizable/jagged anonymous dimension
                                int:            fixed-shape anonymous dimension
                                string:         named dimension

                        Chunking is VERY important for performance. Small chunks such as (10,100) or even (1, 100) can be an order of magnitude 
                        faster for random access, but an order of magnitude slower for contiguous access, as compared to large chunks 
                        like (100, 1000) or (1000, 1000). If you know that you will only read in large contiguous blocks, use large chunks 
                        along those dimensions. If you know you will be reading many randomly placed single or few indices, use small chunks 
                        along those dimensions.

                        For rank-0 tensors, use chunks=(1,)
                &#34;&#34;&#34;
                self.dtype = dtype
                
                # Check that the type is valid
                if dtype not in Tensor.valid_types:
                        raise TypeError(f&#34;Invalid Tensor type {dtype}&#34;)

                self.dims = dims
                self.jagged = jagged

                self.name = &#34;&#34;  # Will be set if the Tensor is read from the db
                self.wsm: Optional[shoji.WorkspaceManager] = None  # Will be set if the Tensor is read from the db

                if inits is None:
                        self.inits: Optional[TensorValue] = None
                        self.shape = (0,) * len(dims)
                else:
                        # If scalar, convert to an ndarray scalar which will have shape ()
                        if np.isscalar(inits):
                                self.inits = TensorValue(np.array(inits, dtype=self.numpy_dtype()))
                        else:
                                self.inits = TensorValue(inits)
                        if self.inits.jagged and not self.jagged:
                                raise ValueError(f&#34;Jagged inits cannot be used to create non-jagged tensor&#34;)
                        self.shape = self.inits.shape

                        if len(self.dims) != len(self.shape):
                                raise ValueError(f&#34;Rank mismatch: shape {self.dims} declared is not the same rank as shape {self.shape} of values&#34;)

                        if self.dtype != self.inits.dtype:
                                raise TypeError(f&#34;Tensor dtype &#39;{self.dtype}&#39; does not match dtype of inits &#39;{self.inits.dtype}&#39;&#34;)

                for ix, dim in enumerate(self.dims):
                        if dim is not None and not isinstance(dim, int) and not isinstance(dim, str):
                                raise ValueError(f&#34;Dimension {ix} &#39;{dim}&#39; is invalid (must be None, int or str)&#34;)

                        if isinstance(dim, int) and self.inits is not None:
                                if self.shape[ix] != dim:  # type: ignore
                                        raise IndexError(f&#34;Mismatch between the declared shape {dim}Â of dimension {ix} and the inferred shape {self.shape} of values&#34;)

                self.chunks: Tuple[int, ...] = ()
                if chunks is None:
                        if dtype in (&#34;bool&#34;, &#34;uint8&#34;, &#34;int8&#34;):
                                byte_size = 1
                        if dtype in (&#34;uint16&#34;, &#34;int16&#34;, &#34;float16&#34;):
                                byte_size = 2
                        elif dtype in (&#34;uint32&#34;, &#34;int32&#34;, &#34;float32&#34;):
                                byte_size = 4
                        elif dtype in (&#34;uint64&#34;, &#34;int64&#34;, &#34;float64&#34;):
                                byte_size = 8
                        elif dtype == &#34;string&#34;:
                                byte_size = 32  # This will fail for very long strings
                        if self.rank == 0:
                                self.chunks = ()
                        elif self.rank == 1:
                                self.chunks = (500 // byte_size,)
                        else:
                                desired_sizes = (300 // byte_size, 100) + (1,) * (self.rank - 2)
                                max_sizes = (dim if isinstance(dim, int) else sys.maxsize for dim in self.dims)
                                self.chunks = tuple(min(a,b) for a,b in zip(max_sizes, desired_sizes))
                else:
                        if len(chunks) != self.rank:
                                raise ValueError(f&#34;chunks={chunks} is wrong number of dimensions for rank-{self.rank} tensor&#34; + (&#34; (use () for rank-0 tensor)&#34; if self.rank == 0 else &#34;&#34;))
                        self.chunks = chunks
                self.initializing = False

        # Support pickling
        def __getstate__(self):
                &#34;&#34;&#34;Return state values to be pickled.&#34;&#34;&#34;
                return (self.dtype, self.jagged, self.dims, self.shape, self.chunks, self.initializing, 0)  # The extra zero is for future use as a version flag

        def __setstate__(self, state):
                &#34;&#34;&#34;Restore state from the unpickled state values.&#34;&#34;&#34;
                self.dtype, self.jagged, self.dims, self.shape, self.chunks, self.initializing, _ = state

        def __len__(self) -&gt; int:
                if self.rank &gt; 0:
                        return self.shape[0]
                return 0

        @property
        def rank(self) -&gt; int:
                return len(self.dims)

        @property
        def bytewidth(self) -&gt; int:
                if self.dtype in (&#34;bool&#34;, &#34;uint8&#34;, &#34;int8&#34;):
                        return 1
                elif self.dtype in (&#34;uint16&#34;, &#34;int16&#34;, &#34;float16&#34;):
                        return 2
                elif self.dtype in (&#34;uint32&#34;, &#34;int32&#34;, &#34;float32&#34;):
                        return 3
                elif self.dtype in (&#34;uint64&#34;, &#34;int64&#34;, &#34;float64&#34;):
                        return 4
                return -1

        def _fancy_indexing(self, expr: FancyIndex) -&gt; Tuple[&#34;shoji.Filter&#34;, ...]:
                if isinstance(expr, tuple):
                        fancyindex: Tuple[FancyIndexElement, ...] = expr
                else:
                        fancyindex = (expr,)
        
                # Fill in missing axes with : just like numpy does
                if any(isinstance(x, type(...)) for x in fancyindex):  # We can&#39;t use a simple &#34;if ... in fancyindex&#34; because fancyindex may contain numpy arrays which complain about ==
                        ix = fancyindex.index(...)
                        fancyindex = fancyindex[:ix] + (slice(None),) * (self.rank - len(fancyindex) - 1) + fancyindex[ix + 1:]

                if len(fancyindex) &lt; self.rank:
                        fancyindex += (slice(None),) * (self.rank - len(fancyindex))

                filters: List[shoji.Filter] = []
                for axis, (dim, fi) in enumerate(zip(self.dims, fancyindex)):
                        # Maybe it&#39;s a Filter?
                        if isinstance(fi, shoji.Filter):
                                if isinstance(dim, str) and isinstance(fi.dim, str) and fi.dim != dim:
                                        raise IndexError(f&#34;Tensor dimension &#39;{dim}&#39; cannot be indexed uing filter expression &#39;{fi}&#39; with dimension &#39;{fi.dim}&#39;&#34;)
                                filters.append(fi)
                        elif isinstance(fi, slice):
                                filters.append(shoji.TensorSliceFilter(self, fi, axis))
                        elif isinstance(fi, (int, np.int64, np.int32)):
                                filters.append(shoji.TensorIndicesFilter(self, np.array(fi), axis))
                        elif isinstance(fi, np.ndarray):
                                if np.issubdtype(fi.dtype, np.bool_):
                                        filters.append(shoji.TensorBoolFilter(self, fi, axis))
                                elif np.issubdtype(fi.dtype, np.int_):
                                        filters.append(shoji.TensorIndicesFilter(self, fi, axis))
                        else:
                                raise KeyError()
                return tuple(filters)

        def __getitem__(self, expr: FancyIndex) -&gt; np.ndarray:
                assert self.wsm is not None, &#34;Tensor is not bound to a database&#34;
                return shoji.View(self.wsm, self._fancy_indexing(expr))[self.name]

        def __setitem__(self, expr: FancyIndex, vals: np.ndarray) -&gt; None:
                assert self.wsm is not None, &#34;Tensor is not bound to a database&#34;
                shoji.View(self.wsm, self._fancy_indexing(expr))[self.name] = vals

        def numpy_dtype(self) -&gt; str:
                if self.dtype == &#34;string&#34;:
                        return &#34;object&#34;
                return self.dtype

        def python_dtype(self) -&gt; Callable:
                if self.dtype == &#34;string&#34;:
                        return str
                if self.dtype == &#34;bool&#34;:
                        return bool
                if self.dtype in (&#34;float16&#34;, &#34;float32&#34;, &#34;float64&#34;):
                        return float
                return int

        def _compare(self, operator, other) -&gt; &#34;shoji.Filter&#34;:
                if isinstance(other, Tensor):
                        return shoji.TensorFilter(operator, self, other)
                elif isinstance(other, (str, int, float, bool)):
                        return shoji.ConstFilter(operator, self, other)
                elif isinstance(other, np.integer):
                        return shoji.ConstFilter(operator, self, int(other))
                elif isinstance(other, np.float):
                        return shoji.ConstFilter(operator, self, float(other))
                elif isinstance(other, np.object):
                        return shoji.ConstFilter(operator, self, str(other))
                elif isinstance(other, np.bool):
                        return shoji.ConstFilter(operator, self, bool(other))
                else:
                        raise TypeError(&#34;Invalid operands for expression&#34;)

        def __eq__(self, other) -&gt; &#34;shoji.Filter&#34;:  # type: ignore
                return self._compare(&#34;==&#34;, other)
                
        def __ne__(self, other) -&gt; &#34;shoji.Filter&#34;:  # type: ignore
                return self._compare(&#34;!=&#34;, other)

        def __gt__(self, other) -&gt; &#34;shoji.Filter&#34;:  # type: ignore
                return self._compare(&#34;&gt;&#34;, other)

        def __lt__(self, other) -&gt; &#34;shoji.Filter&#34;:  # type: ignore
                return self._compare(&#34;&lt;&#34;, other)

        def __ge__(self, other) -&gt; &#34;shoji.Filter&#34;:  # type: ignore
                return self._compare(&#34;&gt;=&#34;, other)

        def __le__(self, other) -&gt; &#34;shoji.Filter&#34;:  # type: ignore
                return self._compare(&#34;&lt;=&#34;, other)

        def append(self, vals: Union[List[np.ndarray], np.ndarray], axis: int = 0) -&gt; None:
                assert self.wsm is not None, &#34;Cannot append to unbound tensor&#34;
                
                if self.rank == 0:
                        raise ValueError(&#34;Cannot append to a scalar&#34;)

                tv = TensorValue(vals)
                shoji.io.append_values_multibatch(self.wsm, [self.name], [tv], (axis,))

        def _quick_look(self) -&gt; str:
                if self.rank == 0:
                        if self.dtype == &#34;string&#34;:
                                s = f&#39;&#34;{self[:]}&#34;&#39;
                        else:
                                s = str(self[:])
                        if len(s) &gt; 60:
                                return s[:56] + &#34; ...&#34;
                        return s

                def look(vals) -&gt; str:
                        s = &#34;[&#34;
                        if not isinstance(vals, list) and vals.ndim == 1:
                                if self.dtype == &#34;string&#34;:
                                        s += &#34;, &#34;.join([f&#39;&#34;{x}&#34;&#39; for x in vals[:5]])
                                else:
                                        s += &#34;, &#34;.join([str(x) for x in vals[:5]])
                        else:
                                elms = []
                                for val in vals[:5]:
                                        elms.append(look(val))
                                s += &#34;, &#34;.join(elms)
                        if len(vals) &gt; 5:
                                s += &#34;, ...]&#34;
                        else:
                                s += &#34;]&#34;
                        return s

                s = look(self[:10])
                if len(s) &gt; 60:
                        return s[:56] + &#34; Â·Â·Â·&#34;
                return s



        def __repr__(self) -&gt; str:
                return f&#34;&lt;Tensor {self.name} dtype=&#39;{self.dtype}&#39; dims={self.dims}, shape={self.shape}, chunks={self.chunks}&gt;&#34;</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="shoji.tensor.Tensor.valid_types"><code class="name">var <span class="ident">valid_types</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="shoji.tensor.Tensor.bytewidth"><code class="name">var <span class="ident">bytewidth</span> :Â int</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def bytewidth(self) -&gt; int:
        if self.dtype in (&#34;bool&#34;, &#34;uint8&#34;, &#34;int8&#34;):
                return 1
        elif self.dtype in (&#34;uint16&#34;, &#34;int16&#34;, &#34;float16&#34;):
                return 2
        elif self.dtype in (&#34;uint32&#34;, &#34;int32&#34;, &#34;float32&#34;):
                return 3
        elif self.dtype in (&#34;uint64&#34;, &#34;int64&#34;, &#34;float64&#34;):
                return 4
        return -1</code></pre>
</details>
</dd>
<dt id="shoji.tensor.Tensor.rank"><code class="name">var <span class="ident">rank</span> :Â int</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def rank(self) -&gt; int:
        return len(self.dims)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="shoji.tensor.Tensor.append"><code class="name flex">
<span>def <span class="ident">append</span></span>(<span>self, vals:Â Union[List[numpy.ndarray],Â numpy.ndarray], axis:Â intÂ =Â 0) â€‘>Â NoneType</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def append(self, vals: Union[List[np.ndarray], np.ndarray], axis: int = 0) -&gt; None:
        assert self.wsm is not None, &#34;Cannot append to unbound tensor&#34;
        
        if self.rank == 0:
                raise ValueError(&#34;Cannot append to a scalar&#34;)

        tv = TensorValue(vals)
        shoji.io.append_values_multibatch(self.wsm, [self.name], [tv], (axis,))</code></pre>
</details>
</dd>
<dt id="shoji.tensor.Tensor.numpy_dtype"><code class="name flex">
<span>def <span class="ident">numpy_dtype</span></span>(<span>self) â€‘>Â str</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def numpy_dtype(self) -&gt; str:
        if self.dtype == &#34;string&#34;:
                return &#34;object&#34;
        return self.dtype</code></pre>
</details>
</dd>
<dt id="shoji.tensor.Tensor.python_dtype"><code class="name flex">
<span>def <span class="ident">python_dtype</span></span>(<span>self) â€‘>Â Callable</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def python_dtype(self) -&gt; Callable:
        if self.dtype == &#34;string&#34;:
                return str
        if self.dtype == &#34;bool&#34;:
                return bool
        if self.dtype in (&#34;float16&#34;, &#34;float32&#34;, &#34;float64&#34;):
                return float
        return int</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="shoji.tensor.TensorValue"><code class="flex name class">
<span>class <span class="ident">TensorValue</span></span>
<span>(</span><span>values:Â Union[Tuple[numpy.ndarray],Â List[numpy.ndarray],Â numpy.ndarray])</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TensorValue:
        def __init__(self, values: Union[Tuple[np.ndarray], List[np.ndarray], np.ndarray]) -&gt; None:
                self.values = values
                if isinstance(values, (list, tuple)):
                        self.jagged = True
                        self.rank = values[0].ndim + 1
                        self.dtype = values[0].dtype.name
                        if self.dtype == &#34;object&#34;:
                                self.dtype = &#34;string&#34;

                        shape = np.array(values[0].shape)
                        for i, array in enumerate(values):
                                if not isinstance(array, np.ndarray):
                                        raise ValueError(&#34;Rows of jagged tensor must be numpy ndarrays&#34;)
                                if self.rank != array.ndim + 1:
                                        raise ValueError(&#34;Rows of jagged tensor cannot be mixed rank&#34;)
                                if self.dtype != array.dtype:
                                        raise ValueError(&#34;Rows of jagged tensor cannot be mixed dtype&#34;)
                                if self.dtype == &#34;string&#34;:
                                        if not all([isinstance(x, str) for x in array.flat]):
                                                raise TypeError(&#34;string tensors (numpy dtype=&#39;object&#39;) must contain only string elements&#34;)
                                if array.ndim != len(shape):
                                        raise ValueError(f&#34;Rank mismatch: shape {array.shape} of subarray at row {i} is not the same rank as shape {shape} at row 0&#34;)
                                shape = np.maximum(shape, array.shape)
                        self.shape = tuple([len(values)] + list(shape))
                else:
                        self.jagged = False
                        self.rank = values.ndim
                        self.dtype = values.dtype.name
                        if self.dtype == &#34;object&#34;:
                                self.dtype = &#34;string&#34;
                                if not all([isinstance(x, str) for x in values.flat]):
                                        raise TypeError(&#34;string tensors (numpy dtype=&#39;object&#39;) must contain only string elements&#34;)
                        self.shape = values.shape

                if self.dtype not in Tensor.valid_types:
                        raise TypeError(f&#34;Invalid dtype &#39;{self.dtype}&#39; for tensor value&#34;)

        @property
        def size(self) -&gt; int:
                return np.prod(self.shape)

        def __len__(self) -&gt; int:
                if self.rank &gt; 0:
                        return self.shape[0]
                return 1
        
        def __iter__(self):
                for row in self.values:
                        yield row

        def __getitem__(self, slice_) -&gt; &#34;TensorValue&#34;:
                if self.jagged:
                        if isinstance(slice_, slice):
                                slice_ = (slice_)
                        slice_ = slice_ + (slice(None),) * (self.rank - len(slice_))
                        sliced = [vals[slice_[1:]] for vals in self.values[slice_[0]]]
                        return TensorValue(sliced)
                return TensorValue(self.values[slice_])

        def size_in_bytes(self) -&gt; int:
                n_bytes = 0
                if not self.jagged:
                        if self.dtype == &#34;string&#34;:
                                n_bytes += sum([len(s) + 1 for s in self.values]) * 2
                        else:
                                n_bytes += self.values.size * self.values.itemsize  # type: ignore
                else:
                        for row in self.values:
                                if self.dtype == &#34;string&#34;:
                                        n_bytes += sum([len(s) + 1 for s in row]) * 2
                                else:
                                        n_bytes += row.size * row.itemsize
                return n_bytes</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="shoji.tensor.TensorValue.size"><code class="name">var <span class="ident">size</span> :Â int</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def size(self) -&gt; int:
        return np.prod(self.shape)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="shoji.tensor.TensorValue.size_in_bytes"><code class="name flex">
<span>def <span class="ident">size_in_bytes</span></span>(<span>self) â€‘>Â int</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def size_in_bytes(self) -&gt; int:
        n_bytes = 0
        if not self.jagged:
                if self.dtype == &#34;string&#34;:
                        n_bytes += sum([len(s) + 1 for s in self.values]) * 2
                else:
                        n_bytes += self.values.size * self.values.itemsize  # type: ignore
        else:
                for row in self.values:
                        if self.dtype == &#34;string&#34;:
                                n_bytes += sum([len(s) + 1 for s in row]) * 2
                        else:
                                n_bytes += row.size * row.itemsize
        return n_bytes</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#overview">Overview</a><ul>
<li><a href="#rank">Rank</a></li>
<li><a href="#datatype">Datatype</a></li>
<li><a href="#dimensions">Dimensions</a></li>
<li><a href="#shape">Shape</a></li>
</ul>
</li>
<li><a href="#chunks">Chunks</a></li>
<li><a href="#reading-from-tensors">Reading from tensors</a></li>
<li><a href="#jagged-tensors">Jagged tensors</a></li>
</ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="shoji" href="index.html">shoji</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="shoji.tensor.Tensor" href="#shoji.tensor.Tensor">Tensor</a></code></h4>
<ul class="two-column">
<li><code><a title="shoji.tensor.Tensor.append" href="#shoji.tensor.Tensor.append">append</a></code></li>
<li><code><a title="shoji.tensor.Tensor.bytewidth" href="#shoji.tensor.Tensor.bytewidth">bytewidth</a></code></li>
<li><code><a title="shoji.tensor.Tensor.numpy_dtype" href="#shoji.tensor.Tensor.numpy_dtype">numpy_dtype</a></code></li>
<li><code><a title="shoji.tensor.Tensor.python_dtype" href="#shoji.tensor.Tensor.python_dtype">python_dtype</a></code></li>
<li><code><a title="shoji.tensor.Tensor.rank" href="#shoji.tensor.Tensor.rank">rank</a></code></li>
<li><code><a title="shoji.tensor.Tensor.valid_types" href="#shoji.tensor.Tensor.valid_types">valid_types</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="shoji.tensor.TensorValue" href="#shoji.tensor.TensorValue">TensorValue</a></code></h4>
<ul class="">
<li><code><a title="shoji.tensor.TensorValue.size" href="#shoji.tensor.TensorValue.size">size</a></code></li>
<li><code><a title="shoji.tensor.TensorValue.size_in_bytes" href="#shoji.tensor.TensorValue.size_in_bytes">size_in_bytes</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.4</a>.</p>
</footer>
</body>
</html>